<!DOCTYPE html>
<html>

<head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#">
<meta charset="utf-8" />
<meta http-equiv='X-UA-Compatible' content='IE=edge'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
<title>Khoa há»c dá»¯ liá»‡u</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
<!-- Style for main home page -->
<link rel="stylesheet" href="/assets/css/styles.css">
<link rel="stylesheet" href="/assets/css/styles_toc.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script data-ad-client="ca-pub-4263248182804679" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
<link href="https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300" rel="stylesheet">
<!-- <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet"> -->
<link href="https://fonts.googleapis.com/css?family=Roboto|Source+Sans+Pro" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Fira+Sans" rel="stylesheet">
<link rel="icon" type="image/jpg" href="assets/images/logo.jpg" sizes="32x32">
<link rel="canonical" href="https://phamdinhkhanh.github.io"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="author" content="Pháº¡m ÄÃ¬nh KhÃ¡nh" />
<meta property="og:title" content="" />
<meta property="og:site_name" content="Khanh's blog" />
<meta property="og:url" content="https://phamdinhkhanh.github.io" />
<meta property="og:description" content="" />

<meta property="og:type" content="article" />
<meta property="article:published_time" content="" />


<meta property="article:author" content="Khanh" />
<meta property="article:section" content="" />

<link rel="alternate" type="application/atom+xml" title="Khanh's blog - Atom feed" href="/feed.xml" />
<style>
	
</style>
<!-- -- Import latext  -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
	skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
	inlineMath: [['$','$']]
  }
});
</script>

<!-- Google Analytics -->

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-89509207-1', 'auto');
// ga('send', 'pageview');
ga('send', 'pageview', {
'page': '/',
'title': ''
});
</script>


<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-KTCD8BX');</script>
<!-- End Google Tag Manager -->
</head>
<style>
body {
  padding: 0 7.5%;
}
</style>

<body>
	<div id="fb-root"></div>
	<!-- <script>(function(d, s, id) { -->
	  <!-- var js, fjs = d.getElementsByTagName(s)[0]; -->
	  <!-- if (d.getElementById(id)) return; -->
	  <!-- js = d.createElement(s); js.id = id; -->
	  <!-- js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.9"; -->
	  <!-- fjs.parentNode.insertBefore(js, fjs); -->
	<!-- }(document, 'script', 'facebook-jssdk'));</script> -->
	<br>
	<div content = "container">
		<div class="row">
			<div class="col-md-2 hidden-xs hidden-sm">
				<a  href="/">
					<img width="100%" style="padding-bottom: 3mm;" src="/assets/images/img.jpg" /> </a>
				<br>
				<nav>
					<div class="header">Latest</div>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/05/23/BERTModel.html">BÃ i 36 - BERT model</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/05/05/MultitaskLearning_MultiBranch.html">BÃ i 35 - Multitask Learning - Multi Branch</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/04/22/MultitaskLearning.html">BÃ i 34 - Multitask Learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/04/15/TransferLearning.html">BÃ i 33 - PhÆ°Æ¡ng phÃ¡p Transfer Learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/04/09/TensorflowDataset.html">BÃ i 32 - KÄ© thuáº­t tensorflow Dataset</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/04/03/AWS.html">BÃ i 31 - Amazon Virtual Machine Deep Learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/28/deployTensorflowJS.html">BÃ i 30 - XÃ¢y dá»±ng Web AI trÃªn tensorflow js</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/23/FlaskRestAPI.html">BÃ i 29 - XÃ¢y dá»±ng Flask API cho mÃ´ hÃ¬nh deep learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/21/faceNet.html">BÃ i 28 - Thá»±c hÃ nh training Facenet</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/12/faceNetAlgorithm.html">BÃ i 27 - MÃ´ hÃ¬nh Facenet trong face recognition</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/10/DarknetGoogleColab.html">BÃ i 26 - Huáº¥n luyá»‡n YOLO darknet trÃªn google colab</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/09/DarknetAlgorithm.html">BÃ i 25 - YOLO You Only Look Once</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/02/17/ImbalancedData.html">BÃ i 24 - Máº¥t cÃ¢n báº±ng dá»¯ liá»‡u (imbalanced dataset)</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/02/11/NARSyscom2015.html">BÃ i 23 - Neural Attentive Session-Based Recommendation</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/01/17/ScoreCard.html">BÃ i 22 - Scorecard model</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/01/06/ImagePreprocessing.html">BÃ i 21 - Tiá»n xá»­ lÃ½ áº£nh OpenCV</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/12/26/Sorfmax_Recommendation_Neural_Network.html">BÃ i 20 - Recommendation Neural Network</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/12/12/ARIMAmodel.html">BÃ i 19 - MÃ´ hÃ¬nh ARIMA trong time series</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/12/02/DeepLearningLayer.html">BÃ i 18 - CÃ¡c layers quan trá»ng trong deep learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/11/22/HOG.html">BÃ i 17 - Thuáº­t toÃ¡n HOG (Histrogram of oriented gradient)</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/11/08/RFMModel.html">BÃ i 16 - Model RFM phÃ¢n khÃºc khÃ¡ch hÃ ng</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/11/04/Recommendation_Compound_Part1.html">BÃ i 15 - collaborative vÃ  content-based filtering</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/10/22/googleHeatmap.html">BÃ i 14 - Biá»ƒu Ä‘á»“ trÃªn Google Map</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/10/05/SSDModelObjectDetection.html">BÃ i 13 - Model SSD trong Object Detection</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/09/29/OverviewObjectDetection.html">BÃ i 12 - CÃ¡c thuáº­t toÃ¡n Object Detection</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/09/16/VisualizationPython.html">BÃ i 11 - Visualization trong python</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/09/08/LDATopicModel.html">BÃ i 10 - Thuáº­t toÃ¡n LDA - XÃ¡c Ä‘á»‹nh Topic</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/08/25/PyTorch_Torchtext_Tutorial.html">BÃ i 9 - Pytorch - Buá»•i 3 - torchtext module NLP</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/08/19/CorrectSpellingVietnamseTonePrediction.html">BÃ i 7 - Pytorch - Buá»•i 2 - Seq2seq model correct spelling</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/08/10/PytorchTurtorial1.html">BÃ i 6 - Pytorch - Buá»•i 1 - LÃ m quen vá»›i pytorch</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/07/15/PySparkSQL.html">BÃ i 5 - Model Pipeline - SparkSQL</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/06/18/AttentionLayer.html">BÃ i 4 -  Attention is all you need</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/05/10/Hypothesis_Statistic.html">Apenddix 1 - LÃ½ thuyáº¿t phÃ¢n phá»‘i vÃ  kiá»ƒm Ä‘á»‹nh thá»‘ng kÃª</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/04/29/ModelWord2Vec.html">BÃ i 3 - MÃ´ hÃ¬nh Word2Vec</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/04/22/Ly_thuyet_ve_mang_LSTM.html">BÃ i 2 - LÃ½ thuyáº¿t vá» máº¡ng LSTM part 2</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/01/07/Ky_thuat_feature_engineering.html">BÃ i 1 - KÄ© thuáº­t feature engineering</a></li>
					
				</nav>
			</div>
			<div class="col-md-8 col-xs-12" style="z-index:1">
				<nav class="navbar navbar-inverse" style="background-color: #046897">
					<div class = "container-fluid">
						<div class = "navbar-header>
							<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
								<span class="icon-bar"></span>
								<span class="icon-bar"></span>
								<span class="icon-bar"></span>
							</button>
							<a class="navbar-brand" href="/">
								<span style="color:#FFF">Khoa há»c dá»¯ liá»‡u - Khanh's blog</span>
							</a>
						</div>
						<br>
						<br>
						<div class="collapse navbar-collapse navbar-right" id="myNavbar">
							<ul class="nav navbar-nav">
								<li><a href="/home"><span style="color: #fff"> Home</span></a></li>
								<li><a href="/about"><span style="color: #fff"> About</span></a></li>
								<!-- <li><a href="/da"><span style="color: #fff">Data Analytics</span></a></li> -->
								<!-- <li><a href="/cv"><span style="color: #fff">Computer Vision</span></a></li> -->
								<!-- <li><a href="/nlp"><span style="color: #fff">NLP</span></a></li> -->
								<!-- <li><a href="/code"><span style="color: #fff">Code</span></a></li> -->
								<li><a href="/book"><span style="color: #fff">Book</span></a></li>
							</ul>
						</div>
					</div>
				</nav>
				<div class="PageNavigation">
				</div>
				<h1 itemprop="name" class="post-title"></h1>
				<div id="bootstrap-overrides">
					<div>
<h2><p class="post-link" style="text-align: left; color: #204081; font-weight: bold">BÃ i 36 - BERT model</p></h2> 
<strong>23 May 2020 - phamdinhkhanh</strong>
</div>
<br/>
<div id="toc"></div>
<h1 id="1-giá»›i-thiá»‡u-chung">1. Giá»›i thiá»‡u chung</h1>

<h2 id="11-má»™t-sá»‘-khÃ¡i-niá»‡m">1.1. Má»™t sá»‘ khÃ¡i niá»‡m</h2>

<p>TrÆ°á»›c khi Ä‘i vÃ o bÃ i nÃ y, chÃºng ta cáº§n hiá»ƒu rÃµ má»™t sá»‘ khÃ¡i niá»‡m:</p>

<ul>
  <li>
    <p><strong>Nhiá»‡m vá»¥ phÃ­a sau (Downstream task)</strong>: LÃ  nhá»¯ng tÃ¡c vá»¥ supervised-learning Ä‘Æ°á»£c cáº£i thiá»‡n dá»±a trÃªn nhá»¯ng pretrained model. VD: ChÃºng ta sá»­ dá»¥ng láº¡i cÃ¡c biá»ƒu diá»…n tá»« há»c Ä‘Æ°á»£c tá»« nhá»¯ng pretrained model trÃªn bá»™ vÄƒn báº£n lá»›n vÃ o má»™t tÃ¡c vá»¥ phÃ¢n tÃ­ch cáº£m xÃºc huáº¥n luyá»‡n trÃªn bá»™ vÄƒn báº£n cÃ³ <strong>kÃ­ch thÆ°á»›c nhá» hÆ¡n</strong>. Ãp dá»¥ng pretrain-embedding Ä‘Ã£ giÃºp cáº£i thiá»‡n mÃ´ hÃ¬nh. NhÆ° váº­y tÃ¡c vá»¥ sá»­ dá»¥ng pretrain-embedding Ä‘Æ°á»£c gá»i lÃ  downstream task.</p>
  </li>
  <li>
    <p><strong>Äiá»ƒm khÃ¡i quÃ¡t Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ hiá»ƒu ngÃ´n ngá»¯ (GLUE score benchmark)</strong>: <a href="https://gluebenchmark.com/">GLUE score benchmark</a> lÃ  má»™t táº­p há»£p cÃ¡c chá»‰ sá»‘ Ä‘Æ°á»£c xÃ¢y dá»±ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ khÃ¡i quÃ¡t má»©c Ä‘á»™ hiá»ƒu ngÃ´n ngá»¯ cá»§a cÃ¡c model NLP. CÃ¡c Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cÃ¡c bá»™ dá»¯ liá»‡u tiÃªu chuáº©n Ä‘Æ°á»£c qui Ä‘á»‹nh táº¡i cÃ¡c convention vá» phÃ¡t triá»ƒn vÃ  thÃºc Ä‘áº©y NLP. Má»—i bá»™ dá»¯ liá»‡u tÆ°Æ¡ng á»©ng vá»›i má»™t loáº¡i tÃ¡c NLP vá»¥ nhÆ°: PhÃ¢n tÃ­ch cáº£m xÃºc (Sentiment Analysis), há»i Ä‘Ã¡p (Question and Answering), dá»± bÃ¡o cÃ¢u tiáº¿p theo (NSP - Next Sentence Prediction), nháº­n diá»‡n thá»±c thá»ƒ trong cÃ¢u (NER - Name Entity Recognition), suy luáº­n ngÃ´n ngá»¯ tá»± nhiÃªn (NLI - Natural Languague Inference). Náº¿u báº¡n muá»‘n tÃ¬m hiá»ƒu thÃªm vá» cÃ¡ch tÃ­nh GLUE score vÃ  cÃ¡c bá»™ dá»¯ liá»‡u trong GLUE cÃ³ thá»ƒ Ä‘á»c thÃªm <a href="https://www.tensorflow.org/datasets/catalog/glue">tensorflow - glue</a>.</p>
  </li>
  <li>
    <p><strong>Quan há»‡ vÄƒn báº£n (Textual Entailment)</strong>: LÃ  tÃ¡c vá»¥ Ä‘Ã¡nh giÃ¡ má»‘i quan há»‡ Ä‘á»‹nh hÆ°á»›ng giá»¯a 2 vÄƒn báº£n? NhÃ£n output cá»§a cÃ¡c cáº·p cÃ¢u Ä‘Æ°á»£c chia thÃ nh Ä‘á»‘i láº­p (contradiction), trung láº­p (neutral) hay cÃ³ quan há»‡ Ä‘i kÃ¨m (textual entailment). Cá»¥ thá»ƒ hÆ¡n, chÃºng ta cÃ³ cÃ¡c cÃ¢u:</p>
  </li>
</ul>

<p>A: HÃ´m nay trá»i mÆ°a.</p>

<p>B: TÃ´i mang Ã´ tá»›i trÆ°á»ng.</p>

<p>C: HÃ´m nay trá»i khÃ´ng mÆ°a.</p>

<p>D: HÃ´m nay lÃ  thá»© 3.</p>

<p>Khi Ä‘Ã³ (A, B) cÃ³ má»‘i quan há»‡ Ä‘i kÃ¨m. CÃ¡c cáº·p cÃ¢u (A, C) cÃ³ má»‘i quan há»‡ Ä‘á»•i láº­p vÃ  (A, D) lÃ  trung láº­p.</p>

<ul>
  <li>
    <p><strong>Suy luáº­n ngÃ´n ngá»¯ (Natural Language Inference)</strong>: LÃ  cÃ¡c tÃ¡c vá»¥ suy luáº­n ngÃ´n ngá»¯ Ä‘Ã¡nh giÃ¡ má»‘i quan há»‡ giá»¯a cÃ¡c cáº·p cÃ¢u, cÅ©ng tÆ°Æ¡ng tá»± nhÆ° Textual Entailment.</p>
  </li>
  <li>
    <p><strong>PhÃ¢n tÃ­ch cáº£m xÃºc (Sentiment Analysis)</strong>: PhÃ¢n loáº¡i cáº£m xÃºc vÄƒn báº£n thÃ nh 2 nhÃ£n tÃ­ch cá»±c (positive) vÃ  tiÃªu cá»±c (negative). ThÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c há»‡ thá»‘ng Ä‘Ã¡nh giÃ¡ bÃ¬nh luáº­n cá»§a ngÆ°á»i dÃ¹ng.</p>
  </li>
  <li>
    <p><strong>Há»i Ä‘Ã¡p (Question and Answering)</strong>: LÃ  thuáº­t toÃ¡n há»i vÃ  Ä‘Ã¡p. Äáº§u vÃ o lÃ  má»™t cáº·p cÃ¢u (pair sequence) bao gá»“m: cÃ¢u há»i (question) cÃ³ chá»©c nÄƒng há»i vÃ  Ä‘oáº¡n vÄƒn báº£n (paragraph) chá»©a thÃ´ng tin tráº£ lá»i cho cÃ¢u há»i. Má»™t bá»™ dá»¯ liá»‡u chuáº©n náº±m trong GLUE dataset Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ tÃ¡c vá»¥ há»i vÃ  Ä‘Ã¡p lÃ  <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD - Stanford Question Answering Dataset</a>. ÄÃ¢y lÃ  má»™t bÃ i toÃ¡n khÃ¡ thÃº vá»‹, cÃ¡c báº¡n cÃ³ thá»ƒ xem thÃªm á»©ng dá»¥ng <a href="https://www.facebook.com/TowardDataScience/videos/201232064499053/">Question and Answering - BERT model</a> mÃ  mÃ¬nh Ä‘Ã£ sharing.</p>
  </li>
  <li>
    <p><strong>Ngá»¯ cáº£nh (Contextual)</strong>: LÃ  ngá»¯ cáº£nh cá»§a tá»«. Má»™t tá»« Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi má»™t cÃ¡ch phÃ¡t Ã¢m nhÆ°ng khi Ä‘Æ°á»£c Ä‘áº·t trong nhá»¯ng cÃ¢u khÃ¡c nhau thÃ¬ cÃ³ thá»ƒ mang ngá»¯ nghÄ©a khÃ¡c nhau. ngá»¯ cáº£nh cÃ³ thá»ƒ coi lÃ  mÃ´i trÆ°á»ng xung quanh tá»« Ä‘á»ƒ gÃ³p pháº§n Ä‘á»‹nh nghÄ©a tá»«. VD:</p>
  </li>
</ul>

<p>A: TÃ´i <code class="highlighter-rouge">Ä‘á»“ng</code> Ã½ vá»›i Ã½ kiáº¿n cá»§a anh.</p>

<p>B: LÃ£o Háº¡c pháº£i kiáº¿m tá»«ng <code class="highlighter-rouge">Ä‘á»“ng</code> Ä‘á»ƒ nuÃ´i cáº­u VÃ ng.</p>

<p>ThÃ¬ tá»« <code class="highlighter-rouge">Ä‘á»“ng</code> trong cÃ¢u A vÃ  B cÃ³ Ã½ nghÄ©a khÃ¡c nhau. ChÃºng ta biáº¿t Ä‘iá»u nÃ y vÃ¬ dá»±a vÃ o ngá»¯ cáº£nh cá»§a tá»«.</p>

<ul>
  <li>
    <p><strong>Hiá»‡n Ä‘áº¡i nháº¥t (SOTA)</strong>: state-of-art lÃ  nhá»¯ng phÆ°Æ¡ng phÃ¡p, ká»¹ thuáº­t tá»‘t nháº¥t mang láº¡i hiá»‡u quáº£ cao nháº¥t tá»« trÆ°á»›c Ä‘áº¿n nay.</p>
  </li>
  <li>
    <p><strong>MÃ´ hÃ¬nh biá»ƒu diá»…n mÃ£ hÃ³a 2 chiá»u dá»±a trÃªn biáº¿n Ä‘á»•i (BERT-Bidirectional Encoder Representation from Transformer)</strong>: MÃ´ hÃ¬nh BERT. ÄÃ¢y lÃ  lá»›p mÃ´ hÃ¬nh SOTA trong nhiá»u tÃ¡c vá»¥ cá»§a <code class="highlighter-rouge">GLUE score benchmark</code>.</p>
  </li>
  <li>
    <p><strong>LTR model</strong>: lÃ  mÃ´ hÃ¬nh há»c bá»‘i cáº£nh theo má»™t chiá»u duy nháº¥t tá»« trÃ¡i sang pháº£i. Cháº³ng háº¡n nhÆ° lá»›p cÃ¡c model RNN.</p>
  </li>
  <li>
    <p><strong>MLM (Masked Language Model)</strong>: LÃ  mÃ´ hÃ¬nh mÃ  bá»‘i cáº£nh cá»§a tá»« Ä‘Æ°á»£c há»c tá»« cáº£ 2 phÃ­a bÃªn trÃ¡i vÃ  bÃªn pháº£i cÃ¹ng má»™t lÃºc tá»« nhá»¯ng bá»™ dá»¯ liá»‡u unsupervised text. Dá»¯ liá»‡u input sáº½ Ä‘Æ°á»£c masked (tá»©c thay báº±ng má»™t token MASK) má»™t cÃ¡ch ngáº«u nhiÃªn vá»›i tá»· lá»‡ tháº¥p. Huáº¥n luyá»‡n mÃ´ hÃ¬nh dá»± bÃ¡o tá»« Ä‘Æ°á»£c masked dá»±a trÃªn bá»‘i cáº£nh xung quanh lÃ  nhá»¯ng tá»« khÃ´ng Ä‘Æ°á»£c masked nháº±m tÃ¬m ra biá»ƒu diá»…n cá»§a tá»«.</p>
  </li>
</ul>

<h2 id="12-lÃ½-do-táº¡i-sao-mÃ¬nh-viáº¿t-vá»-bert">1.2. LÃ½ do táº¡i sao mÃ¬nh viáº¿t vá» BERT?</h2>

<p>Táº¡i thá»i Ä‘iá»ƒm mÃ¬nh viáº¿t vá» model BERT thÃ¬ BERT Ä‘Ã£ Ä‘Æ°á»£c ra Ä‘á»i khÃ¡ lÃ¢u. BERT lÃ  model biá»ƒu diá»…n ngÃ´n ngá»¯ Ä‘Æ°á»£c google giá»›i thiá»‡u vÃ o nÄƒm 2018. Táº¡i thá»i Ä‘iá»ƒm cÃ´ng bá»‘, BERT Ä‘Ã£ táº¡o ra má»™t sá»± rung Ä‘á»™ng trong cá»™ng Ä‘á»“ng NLP bá»Ÿi nhá»¯ng cáº£i tiáº¿n chÆ°a tá»«ng cÃ³ á»Ÿ nhá»¯ng model trÆ°á»›c Ä‘Ã³. Trong bÃ i bÃ¡o <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> cÃ¡c tÃ¡c giáº£ Ä‘Ã£ nÃªu ra nhá»¯ng cáº£i tiáº¿n cá»§a model BERT trong cÃ¡c tÃ¡c vá»¥:</p>

<ul>
  <li>
    <p>TÄƒng GLUE score (General Language Understanding Evaluation score), má»™t chá»‰ sá»‘ tá»•ng quÃ¡t Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ hiá»ƒu ngÃ´n ngá»¯ lÃªn <code class="highlighter-rouge">80.5%</code>.</p>
  </li>
  <li>
    <p>TÄƒng accuracy trÃªn bá»™ dá»¯ liá»‡u <a href="https://cims.nyu.edu/~sbowman/multinli/">MultiNLI</a> Ä‘Ã¡nh giÃ¡ tÃ¡c vá»¥ quan há»‡ vÄƒn báº£n (text entailment) lÃªn 86.7%.</p>
  </li>
  <li>
    <p>TÄƒng accuracy F1 score trÃªn bá»™ dá»¯ liá»‡u <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD v1.1</a> Ä‘Ã¡nh giÃ¡ tÃ¡c vá»¥ question and answering lÃªn 93.2%.</p>
  </li>
</ul>

<p>á» thá»i Ä‘iá»ƒm hiá»‡n táº¡i, BERT Ä‘Ã£ Ä‘Æ°á»£c á»©ng dá»¥ng cho Tiáº¿ng Viá»‡t. Báº¡n Ä‘á»c cÃ³ thá»ƒ tham kháº£o dá»± Ã¡n <a href="https://github.com/VinAIResearch/PhoBERT">PhoBERT</a> cá»§a VinAI vá» huáº¥n luyá»‡n trÆ°á»›c biá»ƒu diá»…n tá»« (pre-train word embedding) sá»­ dá»¥ng model BERT. Má»™t sá»‘ báº¡n á»©ng dá»¥ng model PhoBERT vÃ o cÃ¡c tÃ¡c vá»¥ nhÆ° sentiment analysis vÃ  Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ cao nhÆ° <a href="https://github.com/suicao/PhoBert-Sentiment-Classification/">phÃ¢n loáº¡i cáº£m xÃºc bÃ¬nh luáº­n - KhÃ´i Nguyá»…n</a>.</p>

<p>BERT Ä‘Ã£ Ä‘Æ°á»£c ra Ä‘á»i lÃ¢u nhÆ° váº­y vÃ  cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c á»©ng dá»¥ng rá»™ng rÃ£i thÃ¬ táº¡i sao mÃ¬nh láº¡i viáº¿t vá» model nÃ y? ÄÃ³ lÃ  vÃ¬ BERT vÃ  cÃ¡c biáº¿n thá»ƒ mÃ´ hÃ¬nh cá»§a nÃ³ Ä‘ang lÃ  hot trend vÃ  sáº½ Ä‘á»‹nh hÆ°á»›ng cÃ¡c thuáº­t toÃ¡n NLP trong tÆ°Æ¡ng lai.</p>

<h2 id="13-ngá»¯-cáº£nh-contextual-vÃ -vai-trÃ²-trong-nlp">1.3. Ngá»¯ cáº£nh (Contextual) vÃ  vai trÃ² trong NLP</h2>

<p>TrÆ°á»›c khi tÃ¬m hiá»ƒu cÃ¡c ká»¹ thuáº­t Ä‘Ã£ táº¡o ra Æ°u tháº¿ vÆ°á»£t trá»™i cho mÃ´ hÃ¬nh BERT. ChÃºng ta hÃ£y khÃ¡m phÃ¡ vai trÃ² cá»§a ngá»¯ cáº£nh trong NLP.</p>

<p>Báº£n cháº¥t cá»§a ngÃ´n ngá»¯ lÃ  Ã¢m thanh Ä‘Æ°á»£c phÃ¡t ra Ä‘á»ƒ diá»…n giáº£i dÃ²ng suy nghÄ© cá»§a con ngÆ°á»i. Trong giao tiáº¿p, cÃ¡c tá»« thÆ°á»ng khÃ´ng Ä‘á»©ng Ä‘á»™c láº­p mÃ  chÃºng sáº½ Ä‘i kÃ¨m vá»›i cÃ¡c tá»« khÃ¡c Ä‘á»ƒ liÃªn káº¿t máº¡ch láº¡c thÃ nh má»™t cÃ¢u. Hiá»‡u quáº£ biá»ƒu thá»‹ ná»™i dung vÃ  truyá»n Ä‘áº¡t Ã½ nghÄ©a sáº½ lá»›n hÆ¡n so vá»›i tá»«ng tá»« Ä‘á»©ng Ä‘á»™c láº­p.</p>

<p>Ngá»¯ cáº£nh trong cÃ¢u cÃ³ má»™t sá»± áº£nh hÆ°á»Ÿng ráº¥t lá»›n trong viá»‡c giáº£i thÃ­ch Ã½ nghÄ©a cá»§a tá»«. Hiá»ƒu Ä‘Æ°á»£c vai trÃ² máº¥u chá»‘t Ä‘Ã³, cÃ¡c thuáº­t toÃ¡n NLP SOTA Ä‘á»u cá»‘ gáº¯ng Ä‘Æ°a ngá»¯ cáº£nh vÃ o mÃ´ hÃ¬nh nháº±m táº¡o ra sá»± Ä‘á»™t phÃ¡ vÃ  cáº£i tiáº¿n vÃ  mÃ´ hÃ¬nh BERT cÅ©ng nhÆ° váº­y.</p>

<p>PhÃ¢n cáº¥p má»©c Ä‘á»™ phÃ¡t triá»ƒn cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p embedding tá»« trong NLP cÃ³ thá»ƒ bao gá»“m cÃ¡c nhÃ³m:</p>

<p><strong>Non-context (khÃ´ng bá»‘i cáº£nh)</strong>: LÃ  cÃ¡c thuáº­t toÃ¡n khÃ´ng tá»“n táº¡i bá»‘i cáº£nh trong biá»ƒu diá»…n tá»«. ÄÃ³ lÃ  cÃ¡c thuáº­t toÃ¡n NLP Ä‘á»i Ä‘áº§u nhÆ° ` word2vec, GLoVe, fasttext`. ChÃºng ta chá»‰ cÃ³ duy nháº¥t má»™t biá»ƒu diá»…n vÃ©c tÆ¡ cho má»—i má»™t tá»« mÃ  khÃ´ng thay Ä‘á»•i theo bá»‘i cáº£nh. VD:</p>

<p>CÃ¢u A: <code class="highlighter-rouge">ÄÆ¡n vá»‹ tiá»n tá»‡ cá»§a Viá»‡t Nam lÃ  [Ä‘á»“ng]</code></p>

<p>CÃ¢u B: <code class="highlighter-rouge">Vá»£ [Ä‘á»“ng] Ã½ vá»›i Ã½ kiáº¿n cá»§a chá»“ng lÃ  tÄƒng thÃªm má»—i thÃ¡ng 500k tiá»n tiÃªu váº·t</code></p>

<p>ThÃ¬ tá»« Ä‘á»“ng sáº½ mang 2 Ã½ nghÄ©a khÃ¡c nhau nÃªn pháº£i cÃ³ hai biá»ƒu diá»…n tá»« riÃªng biá»‡t. CÃ¡c thuáº­t toÃ¡n non-context Ä‘Ã£ khÃ´ng Ä‘Ã¡p á»©ng Ä‘Æ°á»£c sá»± Ä‘a dáº¡ng vá» ngá»¯ nghÄ©a cá»§a tá»« trong NLP.</p>

<p><strong>Uni-directional (má»™t chiá»u)</strong>: LÃ  cÃ¡c thuáº­t toÃ¡n Ä‘Ã£ báº¯t Ä‘áº§u xuáº¥t hiá»‡n bá»‘i cáº£nh cá»§a tá»«. CÃ¡c phÆ°Æ¡ng phÃ¡p nhÃºng tá»« base trÃªn RNN lÃ  nhá»¯ng phÆ°Æ¡ng phÃ¡p nhÃºng tá»« má»™t chiá»u. CÃ¡c káº¿t quáº£ biá»ƒu diá»…n tá»« Ä‘Ã£ cÃ³ bá»‘i cáº£nh nhÆ°ng chá»‰ Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi má»™t chiá»u tá»« trÃ¡i qua pháº£i hoáº·c tá»« pháº£i qua trÃ¡i. VD:</p>

<p>CÃ¢u C: HÃ´m nay tÃ´i mang 200 tá»· [gá»­i] á»Ÿ ngÃ¢n hÃ ng.</p>

<p>CÃ¢u D: HÃ´m nay tÃ´i mang 200 tá»· [gá»­i] â€¦.</p>

<p>NhÆ° váº­y vÃ©c tÆ¡ biá»ƒu diá»…n cá»§a tá»« <code class="highlighter-rouge">gá»­i</code> Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh thÃ´ng qua cÃ¡c tá»« liá»n trÆ°á»›c vá»›i nÃ³. Náº¿u chá»‰ dá»±a vÃ o cÃ¡c tá»« liá»n trÆ°á»›c <code class="highlighter-rouge">HÃ´m nay tÃ´i mang 200 tá»·</code> thÃ¬ ta cÃ³ thá»ƒ nghÄ© tá»« phÃ¹ há»£p á»Ÿ vá»‹ trÃ­ hiá»‡n táº¡i lÃ  <code class="highlighter-rouge">cho vay, mua, thanh toÃ¡n,...</code>.</p>

<p>VÃ­ dá»¥ Ä‘Æ¡n giáº£n trÃªn Ä‘Ã£ cho tháº¥y cÃ¡c thuáº­t toÃ¡n biá»ƒu diá»…n tá»« cÃ³ bá»‘i cáº£nh tuÃ¢n theo theo má»™t chiá»u sáº½ gáº·p háº¡n cháº¿ lá»›n trong biá»ƒu diá»…n tá»« hÆ¡n so vá»›i biá»ƒu diá»…n 2 chiá»u.</p>

<p>ELMo lÃ  má»™t vÃ­ dá»¥ cho phÆ°Æ¡ng phÃ¡p má»™t chiá»u. Máº·c dÃ¹ ELMo cÃ³ kiáº¿n trÃºc dá»±a trÃªn má»™t máº¡ng BiLSTM xem xÃ©t bá»‘i cáº£nh theo hai chiá»u tá»« trÃ¡i sang pháº£i vÃ  tá»« pháº£i sang trÃ¡i nhÆ°ng nhá»¯ng chiá»u nÃ y lÃ  Ä‘á»™c láº­p nhau nÃªn ta coi nhÆ° Ä‘Ã³ lÃ  biá»ƒu diá»…n má»™t chiá»u.</p>

<p>Thuáº­t toÃ¡n ELMo Ä‘Ã£ cáº£i tiáº¿n hÆ¡n so vá»›i word2vec vÃ  fasttext Ä‘Ã³ lÃ  táº¡o ra nghÄ©a cá»§a tá»« theo bá»‘i cáº£nh. Trong vÃ­ dá»¥ vá» tá»« <code class="highlighter-rouge">Ä‘á»“ng</code> thÃ¬ á»Ÿ má»—i cÃ¢u A vÃ  B chÃºng ta sáº½ cÃ³ má»™t biá»ƒu diá»…n tá»« khÃ¡c biá»‡t.</p>

<p><strong>Bi-directional (hai chiá»u)</strong>: Ngá»¯ nghÄ©a cá»§a má»™t tá»« khÃ´ng chá»‰ Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi nhá»¯ng tá»« liá»n trÆ°á»›c mÃ  cÃ²n Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi toÃ n bá»™ cÃ¡c tá»« xung quanh. Luá»“ng giáº£i thÃ­ch tuÃ¢n theo <strong>Ä‘á»“ng thá»i</strong> tá»« trÃ¡i qua pháº£i vÃ  tá»« pháº£i qua trÃ¡i <strong>cÃ¹ng má»™t lÃºc</strong>. Äáº¡i diá»‡n cho cÃ¡c phÃ©p biá»ƒu diá»…n tá»« nÃ y lÃ  nhá»¯ng mÃ´ hÃ¬nh sá»­ dá»¥ng ká»¹ thuáº­t <code class="highlighter-rouge">transformer</code> mÃ  chÃºng ta sáº½ tÃ¬m hiá»ƒu bÃªn dÆ°á»›i. Gáº§n Ä‘Ã¢y, nhá»¯ng thuáº­t toÃ¡n NLP theo trÆ°á»ng phÃ¡i bidirectional nhÆ° <a href="https://arxiv.org/abs/1810.04805">BERT</a>, <a href="https://arxiv.org/abs/1801.06146">ULMFit</a>, <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">OpenAI GPT</a>  Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng káº¿t quáº£ SOTA trÃªn háº§u háº¿t cÃ¡c tÃ¡c vá»¥ cá»§a <code class="highlighter-rouge">GLUE benchmark</code>.</p>

<h2 id="14-tiáº¿p-cáº­n-nÃ´ng-vÃ -há»c-sÃ¢u-trong-á»©ng-dá»¥ng-pre-training-nlp">1.4. Tiáº¿p cáº­n nÃ´ng vÃ  há»c sÃ¢u trong á»©ng dá»¥ng pre-training NLP</h2>

<h3 id="141-tiáº¿p-cáº­n-nÃ´ng-shallow-approach">1.4.1. Tiáº¿p cáº­n nÃ´ng (shallow approach)</h3>

<p><strong>Imagenet trong Computer Vision</strong></p>

<p>Trong xá»­ lÃ½ áº£nh chÃºng ta Ä‘á»u biáº¿t tá»›i nhá»¯ng pretrained models ná»•i tiáº¿ng trÃªn bá»™ dá»¯ liá»‡u Imagenet vá»›i 1000 classes. Nhá» sá»‘ lÆ°á»£ng classes lá»›n nÃªn háº§u háº¿t cÃ¡c nhÃ£n trong phÃ¢n loáº¡i áº£nh thÃ´ng thÆ°á»ng Ä‘á»u xuáº¥t hiá»‡n trong Imagenet vÃ  chÃºng ta cÃ³ thá»ƒ há»c chuyá»ƒn giao láº¡i cÃ¡c tÃ¡c vá»¥ xá»­ lÃ½ áº£nh ráº¥t nhanh vÃ  tiá»‡n lá»£i. ChÃºng ta cÅ©ng ká»³ vá»ng NLP cÃ³ má»™t tá»£p há»£p cÃ¡c pretrained models nhÆ° váº­y, tri thá»©c tá»« model Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c nguá»“n tÃ i nguyÃªn vÄƒn báº£n khÃ´ng nhÃ£n (unlabeled text) ráº¥t dá»“i dÃ o vÃ  sáºµn cÃ³.</p>

<p><strong>KhÃ³ khÄƒn há»c chuyá»ƒn giao trong NLP</strong></p>

<p>Tuy nhiÃªn trong NLP viá»‡c há»c chuyá»ƒn giao lÃ  khÃ´ng há» Ä‘Æ¡n giáº£n nhÆ° Computer Vision. Táº¡i sao váº­y?</p>

<p>CÃ¡c kiáº¿n trÃºc máº¡ng deep CNN cá»§a Computer Vision cho phÃ©p há»c chuyá»ƒn giao trÃªn Ä‘á»“ng thá»i cáº£ low-level vÃ  high-level features thÃ´ng qua viá»‡c táº­n dá»¥ng láº¡i cÃ¡c tham sá»‘ tá»« nhá»¯ng layers cá»§a mÃ´ hÃ¬nh pretrained.</p>

<p>NhÆ°ng trong NLP, cÃ¡c thuáº­t toÃ¡n cÅ© hÆ¡n nhÆ° <code class="highlighter-rouge">GLoVe, word2vec, fasttext</code> chá»‰ cho phÃ©p sá»­ dá»¥ng cÃ¡c biá»ƒu diá»…n vÃ©c tÆ¡ nhÃºng cá»§a tá»« lÃ  cÃ¡c low-level features nhÆ° lÃ  Ä‘áº§u vÃ o cho layer Ä‘áº§u tiÃªn cá»§a mÃ´ hÃ¬nh. CÃ¡c layers cÃ²n láº¡i giÃºp táº¡o ra high-level features thÃ¬ dÆ°á»ng nhÆ° Ä‘Æ°á»£c huáº¥n luyá»‡n láº¡i tá»« Ä‘áº§u.</p>

<p>NhÆ° váº­y chÃºng ta chá»‰ chuyá»ƒn giao Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng á»Ÿ má»©c Ä‘á»™ ráº¥t nÃ´ng nÃªn phÆ°Æ¡ng phÃ¡p nÃ y cÃ²n Ä‘Æ°á»£c gá»i lÃ  tiáº¿p cáº­n nÃ´ng (shallow approach). Viá»‡c tiáº¿p cáº­n vá»›i cÃ¡c layers sÃ¢u hÆ¡n lÃ  khÃ´ng thá»ƒ. Äiá»u nÃ y táº¡o ra má»™t háº¡n cháº¿ ráº¥t lá»›n Ä‘á»‘i vá»›i NLP so vá»›i Computer Vision trong viá»‡c há»c chuyá»ƒn giao. CÃ¡ch tiáº¿p cáº­n nÃ´ng trong há»c chuyá»ƒn giao cÃ²n Ä‘Æ°á»£c xem nhÆ° lÃ  <strong>feature-based</strong>.</p>

<p>Khi Ã¡p dá»¥ng feature-based, chÃºng ta sáº½ táº­n dá»¥ng láº¡i cÃ¡c biá»ƒu diá»…n tá»« Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn nhá»¯ng kiáº¿n trÃºc mÃ´ hÃ¬nh cá»‘ Ä‘á»‹nh vÃ  nhá»¯ng bá»™ vÄƒn báº£n cÃ³ kÃ­ch thÆ°á»›c <strong>ráº¥t lá»›n</strong> Ä‘á»ƒ nÃ¢ng cao kháº£ nÄƒng biá»ƒu diá»…n tá»« trong khÃ´ng gian Ä‘a chiá»u. Má»™t sá»‘ pretrained feature-based báº¡n cÃ³ thá»ƒ Ã¡p dá»¥ng trong tiáº¿ng anh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n sáºµn Ä‘Ã³ lÃ  GloVe, <a href="https://wikipedia2vec.github.io/wikipedia2vec/pretrained/">word2vec</a>, <a href="https://fasttext.cc/docs/en/english-vectors.html">fasttext</a>, <a href="https://arxiv.org/abs/1802.05365">ELMo</a>.</p>

<h3 id="142-há»c-sÃ¢u-deep-learning">1.4.2. Há»c sÃ¢u (deep-learning)</h3>

<p>CÃ¡c mÃ´ hÃ¬nh NLP Ä‘á»™t phÃ¡ trong hai nÄƒm trá»Ÿ láº¡i Ä‘Ã¢y nhÆ° <a href="https://arxiv.org/abs/1810.04805">BERT</a>, <a href="https://arxiv.org/pdf/1802.05365">ELMo</a>, <a href="https://arxiv.org/abs/1801.06146">ULMFit</a>, <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">OpenAI GPT</a> Ä‘Ã£ cho phÃ©p viá»‡c chuyá»ƒn giao layers trong NLP kháº£ thi hÆ¡n.</p>

<p>ChÃºng ta khÃ´ng chá»‰ há»c chuyá»ƒn giao Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng mÃ  cÃ²n chuyá»ƒn giao Ä‘Æ°á»£c kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh nhá» sá»‘ lÆ°á»£ng layers nhiá»u hÆ¡n, chiá»u sÃ¢u cá»§a mÃ´ hÃ¬nh sÃ¢u hÆ¡n trÆ°Æ¡c Ä‘Ã³.</p>

<p>CÃ¡c kiáº¿n trÃºc má»›i phÃ¢n cáº¥p theo level cÃ³ kháº£ nÄƒng chuyá»ƒn giao Ä‘Æ°á»£c nhá»¯ng cáº¥p Ä‘á»™ khÃ¡c nhau cá»§a Ä‘áº·c trÆ°ng tá»« low-level tá»›i high-level. Trong khi há»c nÃ´ng chá»‰ chuyá»ƒn giao Ä‘Æ°á»£c low-level táº¡i layer Ä‘áº§u tiÃªn. Táº¥t nhiÃªn low-level cÅ©ng Ä‘Ã³ng vai trÃ² quan trá»ng trong cÃ¡c tÃ¡c vá»¥ NLP. NhÆ°ng high-level lÃ  nhá»¯ng Ä‘áº·c trÆ°ng cÃ³ Ã½ nghÄ©a hÆ¡n vÃ¬ Ä‘Ã³ lÃ  nhá»¯ng Ä‘áº·c trÆ°ng Ä‘Ã£ Ä‘Æ°á»£c tinh luyá»‡n.</p>

<p>NgÆ°á»i ta ká»³ vá»ng ráº±ng <code class="highlighter-rouge">ULMFit, OpenAI GPT, BERT</code> sáº½ lÃ  nhá»¯ng mÃ´ hÃ¬nh pretrained giÃºp tiáº¿n gáº§n hÆ¡n tá»›i viá»‡c xÃ¢y dá»±ng má»™t lá»›p cÃ¡c pretrained models <code class="highlighter-rouge">ImageNet for NLP</code>. CÃ¡c báº¡n cÃ³ thá»ƒ xem thÃªm Ã½ tÆ°á»Ÿng vá» xÃ¢y dá»±ng <a href="https://ruder.io/nlp-imagenet/">Imagenet for NLP</a>.</p>

<p>Khi há»c chuyá»ƒn giao theo phÆ°Æ¡ng phÃ¡p há»c sÃ¢u chÃºng ta sáº½ táº­n dá»¥ng láº¡i kiáº¿n trÃºc tá»« mÃ´ hÃ¬nh pretrained vÃ  bá»• sung má»™t sá»‘ layers phÃ­a sau Ä‘á»ƒ phÃ¹ há»£p vá»›i nhiá»‡m vá»¥ huáº¥n luyá»‡n. CÃ¡c tham sá»‘ cá»§a cÃ¡c layers gá»‘c sáº½ Ä‘Æ°á»£c <strong>fine-tunning</strong> láº¡i. Chá»‰ má»™t sá»‘ Ã­t cÃ¡c tham sá»‘ á»Ÿ layers bá»• sung Ä‘Æ°á»£c huáº¥n luyá»‡n láº¡i tá»« Ä‘áº§u. Báº¡n Ä‘á»c cÃ³ thá»ƒ tÃ¬m hiá»ƒu thÃªm vá» fine-tuning táº¡i <a href="https://phamdinhkhanh.github.io/2020/04/15/TransferLearning.html">BÃ i 33 - PhÆ°Æ¡ng phÃ¡p Transfer Learning</a>.</p>

<h2 id="21-phÆ°Æ¡ng-phÃ¡p-transformer">2.1. PhÆ°Æ¡ng phÃ¡p transformer</h2>

<h3 id="211-encoder-vÃ -decoder-trong-bert">2.1.1. Encoder vÃ  decoder trong BERT</h3>

<p>TrÆ°á»›c khi hiá»ƒu vá» BERT chÃºng ta cÃ¹ng Ã´n láº¡i vá» ká»¹ thuáº­t transformer. MÃ¬nh Ä‘Ã£ diá»…n giáº£i ká»¹ thuáº­t nÃ y táº¡i <a href="https://phamdinhkhanh.github.io/2019/06/18/AttentionLayer.html">BÃ i 4 - Attention is all you need</a>. ÄÃ¢y lÃ m má»™t lá»›p mÃ´ hÃ¬nh seq2seq gá»“m 2 phrase encoder vÃ  decoder. MÃ´ hÃ¬nh hoÃ n toÃ n khÃ´ng sá»­ dá»¥ng cÃ¡c kiáº¿n trÃºc Recurrent Neural Network cá»§a RNN mÃ  chá»‰ sá»­ dá»¥ng cÃ¡c layers attention Ä‘á»ƒ embedding cÃ¡c tá»« trong cÃ¢u. Kiáº¿n trÃºc cá»¥ thá»ƒ cá»§a mÃ´ hÃ¬nh nhÆ° sau:</p>

<p><img src="/assets/images/20200523_BERTModel/pic1.png" class="largepic" /></p>

<p><strong>HÃ¬nh 1:</strong> SÆ¡ Ä‘á»“ kiáº¿n trÃºc transformer káº¿t há»£p vá»›i attention. Nguá»“n <a href="https://arxiv.org/abs/1706.03762">attention is all you need</a>.</p>

<p>MÃ´ hÃ¬nh sáº½ bao gá»“m 2 phase.</p>

<ul>
  <li>
    <p><strong>Encoder</strong>: Bao gá»“m 6 layers liÃªn tiáº¿p nhau. Má»—i má»™t layer sáº½ bao gá»“m má»™t sub-layer lÃ  Multi-Head Attention káº¿t há»£p vá»›i fully-connected layer nhÆ° mÃ´ táº£ á»Ÿ nhÃ¡nh encoder bÃªn trÃ¡i cá»§a hÃ¬nh váº½. Káº¿t thÃºc quÃ¡ trÃ¬nh encoder ta thu Ä‘Æ°á»£c má»™t vector embedding output cho má»—i tá»«.</p>
  </li>
  <li>
    <p><strong>Decoder</strong>: Kiáº¿n trÃºc cÅ©ng bao gá»“m cÃ¡c layers liÃªn tiáº¿p nhau. Má»—i má»™t layer cá»§a Decoder cÅ©ng cÃ³ cÃ¡c sub-layers gáº§n tÆ°Æ¡ng tá»± nhÆ° layer cá»§a Encoder nhÆ°ng bá»• sung thÃªm sub-layer Ä‘áº§u tiÃªn lÃ  <code class="highlighter-rouge">Masked Multi-Head Attention</code> cÃ³ tÃ¡c dá»¥ng loáº¡i bá» cÃ¡c tá»« trong tÆ°Æ¡ng lai khá»i quÃ¡ trÃ¬nh attention.</p>
  </li>
</ul>

<h3 id="212-cÃ¡c-tiáº¿n-trÃ¬nh-self-attention-vÃ -encoder-decoder-attention">2.1.2. CÃ¡c tiáº¿n trÃ¬nh self-attention vÃ  encoder-decoder attention</h3>

<p>Trong kiáº¿n trÃºc transformer chÃºng ta Ã¡p dá»¥ng 2 dáº¡ng attention khÃ¡c nhau táº¡i tá»«ng bÆ°á»›c huáº¥n luyá»‡n.</p>

<p><strong>self-attention</strong>: ÄÆ°á»£c sá»­ dá»¥ng trong cÃ¹ng má»™t cÃ¢u input, táº¡i encoder hoáº·c táº¡i decoder. ÄÃ¢y chÃ­nh lÃ  attention Ä‘Æ°á»£c Ã¡p dá»¥ng táº¡i cÃ¡c Multi-Head Attention á»Ÿ Ä‘áº§u vÃ o cá»§a cáº£ 2 phase encoder vÃ  decoder.</p>

<p><img src="/assets/images/20200523_BERTModel/pic2.png" class="largepic" /></p>

<p><strong>HÃ¬nh 2:</strong> SÆ¡ Ä‘á»“ vá»‹ trÃ­ Ã¡p dá»¥ng self-attention trong kiáº¿n trÃºc transformer. CÃ¡c vÃ©c tÆ¡ embedding cá»§a cÃ¹ng má»™t chuá»—i encoder hoáº·c decoder tá»± liÃªn káº¿t vá»›i nhau Ä‘á»ƒ tÃ­nh toÃ¡n attention nhÆ° hÃ¬nh bÃªn pháº£i.</p>

<p><strong>Encoder-decoder attention</strong>:</p>

<p><img src="/assets/images/20200523_BERTModel/pic3.png" class="largepic" /></p>

<p><strong>HÃ¬nh 3:</strong> BÃªn trÃ¡i lÃ  vá»‹ trÃ­ Ã¡p dá»¥ng encoder-decoder attention. BÃªn pháº£i lÃ  cÃ¡ch tÃ­nh trá»ng sá»‘ attention khi káº¿t há»£p má»—i vÃ©c tÆ¡ embedding á»Ÿ decoder vá»›i toÃ n bá»™ cÃ¡c vÃ©c tÆ¡ embedding á»Ÿ encoder.</p>

<p>Sá»Ÿ dÄ© Ä‘Æ°á»£c gá»i lÃ  encoder-decoder attention vÃ¬ Ä‘Ã¢y lÃ  kiáº¿n trÃºc attention tÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c vÃ©c tÆ¡ embedding cá»§a encoder vÃ  decoder. vÃ©c tÆ¡ context Ä‘Æ°á»£c tÃ­nh toÃ¡n trÃªn encoder Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh tÆ°Æ¡ng quan vá»›i vÃ©c tÆ¡ decoder nÃªn sáº½ cÃ³ Ã½ nghÄ©a giáº£i thÃ­ch bá»‘i cáº£nh cá»§a tá»« táº¡i vá»‹ trÃ­ time step decoder tÆ°Æ¡ng á»©ng. Sau khi káº¿t há»£p giá»¯a vÃ©c tÆ¡ context vÃ  vÃ©c tÆ¡ decoder ta sáº½ project tiáº¿p qua má»™t fully connected layer Ä‘á»ƒ tÃ­nh phÃ¢n phá»‘i xÃ¡c suáº¥t cho output.</p>

<p>Máº·c dÃ¹ cÃ³ kiáº¿n trÃºc chá»‰ gá»“m cÃ¡c biáº¿n Ä‘á»•i attention nhÆ°ng Transformer láº¡i cÃ³ káº¿t quáº£ ráº¥t tá»‘t trong cÃ¡c tÃ¡c vá»¥ NLP nhÆ° sentiment analysis vÃ  dá»‹ch mÃ¡y.</p>

<h1 id="2-giá»›i-thiá»‡u-vá»-bert">2. Giá»›i thiá»‡u vá» BERT</h1>

<p><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a> lÃ  viáº¿t táº¯t cá»§a cá»¥m tá»« <code class="highlighter-rouge">Bidirectional Encoder Representation from Transformer</code> cÃ³ nghÄ©a lÃ  mÃ´ hÃ¬nh biá»ƒu diá»…n tá»« theo 2 chiá»u á»©ng dá»¥ng ká»¹ thuáº­t <code class="highlighter-rouge">Transformer</code>. BERT Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ huáº¥n luyá»‡n trÆ°á»›c cÃ¡c biá»ƒu diá»…n tá»« (pre-train word embedding). Äiá»ƒm Ä‘áº·c biá»‡t á»Ÿ BERT Ä‘Ã³ lÃ  nÃ³ cÃ³ thá»ƒ Ä‘iá»u hÃ²a cÃ¢n báº±ng bá»‘i cáº£nh theo cáº£ 2 chiá»u trÃ¡i vÃ  pháº£i.</p>

<p>CÆ¡ cháº¿ attention cá»§a Transformer sáº½ truyá»n toÃ n bá»™ cÃ¡c tá»« trong cÃ¢u vÄƒn Ä‘á»“ng thá»i vÃ o mÃ´ hÃ¬nh má»™t lÃºc mÃ  khÃ´ng cáº§n quan tÃ¢m Ä‘áº¿n chiá»u cá»§a cÃ¢u. Do Ä‘Ã³ Transformer Ä‘Æ°á»£c xem nhÆ° lÃ  huáº¥n luyá»‡n hai chiá»u (bidirectional) máº·c dÃ¹ trÃªn thá»±c táº¿ chÃ­nh xÃ¡c hÆ¡n chÃºng ta cÃ³ thá»ƒ nÃ³i ráº±ng Ä‘Ã³ lÃ  huáº¥n luyá»‡n khÃ´ng chiá»u (non-directional). Äáº·c Ä‘iá»ƒm nÃ y cho phÃ©p mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c bá»‘i cáº£nh cá»§a tá»« dá»±a trÃªn toÃ n bá»™ cÃ¡c tá»« xung quanh nÃ³ bao gá»“m cáº£ tá»« bÃªn trÃ¡i vÃ  tá»« bÃªn pháº£i.</p>

<h2 id="21-fine-tuning-model-bert">2.1. Fine-tuning model BERT</h2>

<p>Má»™t Ä‘iá»ƒm Ä‘áº·c biá»‡t á»Ÿ BERT mÃ  cÃ¡c model embedding trÆ°á»›c Ä‘Ã¢y chÆ°a tá»«ng cÃ³ Ä‘Ã³ lÃ  káº¿t quáº£ huáº¥n luyá»‡n cÃ³ thá»ƒ fine-tuning Ä‘Æ°á»£c. ChÃºng ta sáº½ thÃªm vÃ o kiáº¿n trÃºc model má»™t output layer Ä‘á»ƒ tÃ¹y biáº¿n theo tÃ¡c vá»¥ huáº¥n luyá»‡n.</p>

<p><img src="/assets/images/20200523_BERTModel/pic4.png" class="largepic" /></p>

<p><strong>HÃ¬nh 4:</strong> ToÃ n bá»™ tiáº¿n trÃ¬nh pre-training vÃ  fine-tuning cá»§a BERT. Má»™t kiáº¿n trÃºc tÆ°Æ¡ng tá»± Ä‘Æ°á»£c sá»­ dá»¥ng cho cáº£ pretrain-model vÃ  fine-tuning model. ChÃºng ta sá»­ dá»¥ng cÃ¹ng má»™t tham sá»‘ pretrain Ä‘á»ƒ khá»Ÿi táº¡o mÃ´ hÃ¬nh cho cÃ¡c tÃ¡c vá»¥ down stream khÃ¡c nhau. Trong suá»‘t quÃ¡ trÃ¬nh fine-tuning thÃ¬ toÃ n bá»™ cÃ¡c tham sá»‘ cá»§a layers há»c chuyá»ƒn giao sáº½ Ä‘Æ°á»£c fine-tune. Äá»‘i vá»›i cÃ¡c tÃ¡c vá»¥ sá»­ dá»¥ng input lÃ  má»™t cáº·p sequence (pair-sequence) vÃ­ dá»¥ nhÆ° <code class="highlighter-rouge">question and answering</code> thÃ¬ ta sáº½ thÃªm token khá»Ÿi táº¡o lÃ  <code class="highlighter-rouge">[CLS]</code> á»Ÿ Ä‘áº§u cÃ¢u, token <code class="highlighter-rouge">[SEP]</code> á»Ÿ giá»¯a Ä‘á»ƒ ngÄƒn cÃ¡ch 2 cÃ¢u.</p>

<p>Tiáº¿n trÃ¬nh Ã¡p dá»¥ng fine-tuning sáº½ nhÆ° sau:</p>

<ul>
  <li>
    <p><strong>BÆ°á»›c 1</strong>: Embedding toÃ n bá»™ cÃ¡c token cá»§a cáº·p cÃ¢u báº±ng cÃ¡c vÃ©c tÆ¡ nhÃºng tá»« pretrain model. CÃ¡c token embedding bao gá»“m cáº£ 2 token lÃ  <code class="highlighter-rouge">[CLS]</code> vÃ  <code class="highlighter-rouge">[SEP]</code> Ä‘á»ƒ Ä‘Ã¡nh dáº¥u vá»‹ trÃ­ báº¯t Ä‘áº§u cá»§a cÃ¢u há»i vÃ  vá»‹ trÃ­ ngÄƒn cÃ¡ch giá»¯a 2 cÃ¢u. 2 token nÃ y sáº½ Ä‘Æ°á»£c dá»± bÃ¡o á»Ÿ output Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c pháº§n <code class="highlighter-rouge">Start/End Spand</code> cá»§a cÃ¢u output.</p>
  </li>
  <li>
    <p><strong>BÆ°á»›c 2</strong>: CÃ¡c embedding vÃ©c tÆ¡ sau Ä‘Ã³ sáº½ Ä‘Æ°á»£c truyá»n vÃ o kiáº¿n trÃºc multi-head attention vá»›i nhiá»u block code (thÆ°á»ng lÃ  6, 12 hoáº·c 24 blocks tÃ¹y theo kiáº¿n trÃºc BERT). Ta thu Ä‘Æ°á»£c má»™t vÃ©c tÆ¡ output á»Ÿ encoder.</p>
  </li>
  <li>
    <p><strong>BÆ°á»›c 3</strong>: Äá»ƒ dá»± bÃ¡o phÃ¢n phá»‘i xÃ¡c suáº¥t cho tá»«ng vá»‹ trÃ­ tá»« á»Ÿ decoder, á»Ÿ má»—i time step chÃºng ta sáº½ truyá»n vÃ o decoder vÃ©c tÆ¡ output cá»§a encoder vÃ  vÃ©c tÆ¡ embedding input cá»§a decoder Ä‘á»ƒ tÃ­nh encoder-decoder attention (cá»¥ thá»ƒ vá» encoder-decoder attention lÃ  gÃ¬ cÃ¡c báº¡n xem láº¡i má»¥c 2.1.1). Sau Ä‘Ã³ projection qua liner layer vÃ  softmax Ä‘á»ƒ thu Ä‘Æ°á»£c phÃ¢n phá»‘i xÃ¡c suáº¥t cho output tÆ°Æ¡ng á»©ng á»Ÿ time step $t$.</p>
  </li>
  <li>
    <p><strong>BÆ°á»›c 4</strong>: Trong káº¿t quáº£ tráº£ ra á»Ÿ output cá»§a transformer ta sáº½ cá»‘ Ä‘á»‹nh káº¿t quáº£ cá»§a cÃ¢u Question sao cho trÃ¹ng vá»›i cÃ¢u Question á»Ÿ input. CÃ¡c vá»‹ trÃ­ cÃ²n láº¡i sáº½ lÃ  thÃ nh pháº§n má»Ÿ rá»™ng <code class="highlighter-rouge">Start/End Span</code> tÆ°Æ¡ng á»©ng vá»›i cÃ¢u tráº£ lá»i tÃ¬m Ä‘Æ°á»£c tá»« cÃ¢u input.</p>
  </li>
</ul>

<p>LÆ°u Ã½ quÃ¡ trÃ¬nh huáº¥n luyá»‡n chÃºng ta sáº½ fine-tune láº¡i toÃ n bá»™ cÃ¡c tham sá»‘ cá»§a model BERT Ä‘Ã£ cut off top linear layer vÃ  huáº¥n luyá»‡n láº¡i tá»« Ä‘áº§u cÃ¡c tham sá»‘ cá»§a linear layer mÃ  chÃºng ta thÃªm vÃ o kiáº¿n trÃºc model BERT Ä‘á»ƒ customize láº¡i phÃ¹ há»£p vá»›i bÃ i toÃ¡n.</p>

<p>NhÆ° váº­y cÃ¡c báº¡n Ä‘Ã£ hÃ¬nh dung Ä‘Æ°á»£c model BERT Ä‘Æ°á»£c fine-tuning trong má»™t tÃ¡c vá»¥ nhÆ° tháº¿ nÃ o rá»“i chá»©? TÃ´i cÃ¡ ráº±ng qua quÃ¡ trÃ¬nh thá»±c hÃ nh á»Ÿ bÃ i sau cÃ¡c báº¡n sáº½ náº¯m vá»¯ng hÆ¡n cÃ¡ch thá»©c fine-tune BERT model.</p>

<h2 id="23-masked-ml-mlm">2.3. Masked ML (MLM)</h2>

<p>Masked ML lÃ  má»™t tÃ¡c vá»¥ cho phÃ©p chÃºng ta fine-tuning láº¡i cÃ¡c biá»ƒu diá»…n tá»« trÃªn cÃ¡c bá»™ dá»¯ liá»‡u unsupervised-text báº¥t ká»³. ChÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng Masked ML cho nhá»¯ng ngÃ´n ngá»¯ khÃ¡c nhau Ä‘á»ƒ táº¡o ra biá»ƒu diá»…n embedding cho chÃºng. CÃ¡c bá»™ dá»¯ liá»‡u cá»§a tiáº¿ng anh cÃ³ kÃ­ch thÆ°á»›c lÃªn tá»›i vÃ i vÃ i trÄƒm tá»›i vÃ i nghÃ¬n GB Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn BERT Ä‘Ã£ táº¡o ra nhá»¯ng káº¿t quáº£ khÃ¡ áº¥n tÆ°á»£ng.</p>

<p>BÃªn dÆ°á»›i lÃ  sÆ¡ Ä‘á»“ huáº¥n luyá»‡n BERT theo tÃ¡c vá»¥ Masked ML</p>

<p><img src="/assets/images/20200523_BERTModel/pic5.png" class="largepic" /></p>

<p><strong>HÃ¬nh 5:</strong> SÆ¡ Ä‘á»“ kiáº¿n trÃºc BERT cho tÃ¡ vá»¥ Masked ML.</p>

<p>Theo Ä‘Ã³:</p>

<ul>
  <li>
    <p>Khoáº£ng 15 % cÃ¡c token cá»§a cÃ¢u input Ä‘Æ°á»£c thay tháº¿ bá»Ÿi <code class="highlighter-rouge">[MASK]</code> token trÆ°á»›c khi truyá»n vÃ o model Ä‘áº¡i diá»‡n cho nhá»¯ng tá»« bá»‹ che dáº¥u (masked). MÃ´ hÃ¬nh sáº½ dá»±a trÃªn cÃ¡c tá»« khÃ´ng Ä‘Æ°á»£c che (non-masked) dáº¥u xung quanh <code class="highlighter-rouge">[MASK]</code> vÃ  Ä‘á»“ng thá»i lÃ  bá»‘i cáº£nh cá»§a <code class="highlighter-rouge">[MASK]</code> Ä‘á»ƒ dá»± bÃ¡o giÃ¡ trá»‹ gá»‘c cá»§a tá»« Ä‘Æ°á»£c che dáº¥u. Sá»‘ lÆ°á»£ng tá»« Ä‘Æ°á»£c che dáº¥u Ä‘Æ°á»£c lá»±a chá»n lÃ  má»™t sá»‘ Ã­t (15%) Ä‘á»ƒ tá»· lá»‡ bá»‘i cáº£nh chiáº¿m nhiá»u hÆ¡n (85%).</p>
  </li>
  <li>
    <p>Báº£n cháº¥t cá»§a kiáº¿n trÃºc BERT váº«n lÃ  má»™t mÃ´ hÃ¬nh seq2seq gá»“m 2 phase encoder giÃºp embedding cÃ¡c tá»« input vÃ  decoder giÃºp tÃ¬m ra phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a cÃ¡c tá»« á»Ÿ output. Kiáº¿n trÃºc Transfomer encoder Ä‘Æ°á»£c giá»¯ láº¡i trong tÃ¡c vá»¥ Masked ML. Sau khi thá»±c hiá»‡n self-attention vÃ  feed forward ta sáº½ thu Ä‘Æ°á»£c cÃ¡c vÃ©c tÆ¡ embedding á»Ÿ output lÃ  $O_1, O_2,â€¦, O_5$</p>
  </li>
  <li>
    <p>Äá»ƒ tÃ­nh toÃ¡n phÃ¢n phá»‘i xÃ¡c suáº¥t cho tá»« output, chÃºng ta thÃªm má»™t Fully connect layer ngay sau Transformer Encoder. HÃ m softmax cÃ³ tÃ¡c dá»¥ng tÃ­nh toÃ¡n phÃ¢n phá»‘i xÃ¡c suáº¥t. Sá»‘ lÆ°á»£ng units cá»§a fully connected layer pháº£i báº±ng vá»›i kÃ­ch thÆ°á»›c cá»§a tá»« Ä‘iá»ƒn.</p>
  </li>
  <li>
    <p>Cuá»‘i cÃ¹ng ta thu Ä‘Æ°á»£c vÃ©c tÆ¡ nhÃºng cá»§a má»—i má»™t tá»« táº¡i vá»‹ trÃ­ MASK sáº½ lÃ  embedding vÃ©c tÆ¡ giáº£m chiá»u cá»§a vÃ©c tÆ¡ $O_i$ sau khi Ä‘i qua fully connected layer nhÆ° mÃ´ táº£ trÃªn hÃ¬nh váº½ bÃªn pháº£i.</p>
  </li>
</ul>

<p>HÃ m loss function cá»§a BERT sáº½ bá» qua máº¥t mÃ¡t tá»« nhá»¯ng tá»« khÃ´ng bá»‹ che dáº¥u vÃ  chá»‰ Ä‘Æ°a vÃ o máº¥t mÃ¡t cá»§a nhá»¯ng tá»« bá»‹ che dáº¥u. Do Ä‘Ã³ mÃ´ hÃ¬nh sáº½ há»™i tá»¥ lÃ¢u hÆ¡n nhÆ°ng Ä‘Ã¢y lÃ  Ä‘áº·c tÃ­nh bÃ¹ trá»« cho sá»± gia tÄƒng Ã½ thá»©c vá» bá»‘i cáº£nh. Viá»‡c lá»±a chá»n ngáº«u nhiÃªn 15% sá»‘ lÆ°á»£ng cÃ¡c tá»« bá»‹ che dáº¥u cÅ©ng táº¡o ra vÃ´ sá»‘ cÃ¡c ká»‹ch báº£n input cho mÃ´ hÃ¬nh huáº¥n luyá»‡n nÃªn mÃ´ hÃ¬nh sáº½ cáº§n pháº£i huáº¥n luyá»‡n ráº¥t lÃ¢u má»›i há»c Ä‘Æ°á»£c toÃ n diá»‡n cÃ¡c kháº£ nÄƒng.</p>

<h2 id="24-next-sentence-prediction-nsp">2.4. Next Sentence Prediction (NSP)</h2>

<p>ÄÃ¢y lÃ  má»™t bÃ i toÃ¡n phÃ¢n loáº¡i há»c cÃ³ giÃ¡m sÃ¡t vá»›i 2 nhÃ£n (hay cÃ²n gá»i lÃ  phÃ¢n loáº¡i nhá»‹ phÃ¢n). Input Ä‘áº§u vÃ o cá»§a mÃ´ hÃ¬nh lÃ  má»™t cáº·p cÃ¢u (pair-sequence) sao cho 50% cÃ¢u thá»© 2 Ä‘Æ°á»£c lá»±a chá»n lÃ  cÃ¢u tiáº¿p theo cá»§a cÃ¢u thá»© nháº¥t vÃ  50% Ä‘Æ°á»£c lá»±a chá»n má»™t cÃ¡ch ngáº«u nhiÃªn tá»« bá»™ vÄƒn báº£n mÃ  khÃ´ng cÃ³ má»‘i liÃªn há»‡ gÃ¬ vá»›i cÃ¢u thá»© nháº¥t. NhÃ£n cá»§a mÃ´ hÃ¬nh sáº½ tÆ°Æ¡ng á»©ng vá»›i <code class="highlighter-rouge">IsNext</code> khi cáº·p cÃ¢u lÃ  liÃªn tiáº¿p hoáº·c <code class="highlighter-rouge">NotNext</code> náº¿u cáº·p cÃ¢u khÃ´ng liÃªn tiáº¿p.</p>

<p>CÅ©ng tÆ°Æ¡ng tá»± nhÆ° mÃ´ hÃ¬nh Question and Answering, chÃºng ta cáº§n Ä‘Ã¡nh dáº¥u cÃ¡c vá»‹ trÃ­ Ä‘áº§u cÃ¢u thá»© nháº¥t báº±ng token <code class="highlighter-rouge">[CLS]</code> vÃ  vá»‹ trÃ­ cuá»‘i cÃ¡c cÃ¢u báº±ng token <code class="highlighter-rouge">[SEP]</code>. CÃ¡c token nÃ y cÃ³ tÃ¡c dá»¥ng nháº­n biáº¿t cÃ¡c vá»‹ trÃ­ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a tá»«ng cÃ¢u thá»© nháº¥t vÃ  thá»© hai.</p>

<p><img src="/assets/images/20200523_BERTModel/pic1.png" class="largepic" /></p>

<p><strong>HÃ¬nh 6:</strong> SÆ¡ Ä‘á»“ kiáº¿n trÃºc model BERT cho tÃ¡c vá»¥ NSP.</p>

<p>ThÃ´ng tin input Ä‘Æ°á»£c preprocessing trÆ°á»›c khi Ä‘Æ°a vÃ o mÃ´ hÃ¬nh huáº¥n luyá»‡n bao gá»“m:</p>

<ul>
  <li>Ngá»¯ nghÄ©a cá»§a tá»« (token embeddings): ThÃ´ng qua cÃ¡c embedding vÃ©c tÆ¡ cho tá»«ng tá»«. CÃ¡c vÃ©c tÆ¡ Ä‘Æ°á»£c khá»Ÿi táº¡o tá»« pretrain model.</li>
</ul>

<p>NgoÃ i embedding biá»ƒu diá»…n tá»« cá»§a cÃ¡c tá»« trong cÃ¢u, mÃ´ hÃ¬nh cÃ²n embedding thÃªm má»™t sá»‘ thÃ´ng tin:</p>

<ul>
  <li>
    <p>Loáº¡i cÃ¢u (segment embeddings): Gá»“m hai vÃ©c tÆ¡ lÃ  $E_A$ náº¿u tá»« thuá»™c cÃ¢u thá»© nháº¥t vÃ  $E_B$ náº¿u tá»« thuá»™c cÃ¢u thá»© hai.</p>
  </li>
  <li>
    <p>Vá»‹ trÃ­ cá»§a tá»« trong cÃ¢u (position embedding): lÃ  cÃ¡c vÃ©c tÆ¡ $E_0, â€¦, E_{10}$. TÆ°Æ¡ng tá»± nhÆ° positional embedding trong transformer.</p>
  </li>
</ul>

<p>VÃ©c tÆ¡ input sáº½ báº±ng tá»•ng cá»§a cáº£ ba thÃ nh pháº§n embedding theo <code class="highlighter-rouge">tá»«, cÃ¢u</code> vÃ  <code class="highlighter-rouge">vá»‹ trÃ­</code>.</p>

<h1 id="3-cÃ¡c-kiáº¿n-trÃºc-model-bert">3. CÃ¡c kiáº¿n trÃºc model BERT</h1>

<p>Hiá»‡n táº¡i cÃ³ nhiá»u phiÃªn báº£n khÃ¡c nhau cá»§a model BERT. CÃ¡c phiÃªn báº£n Ä‘á»u dá»±a trÃªn viá»‡c thay Ä‘á»•i kiáº¿n trÃºc cá»§a Transformer táº­p trung á»Ÿ 3 tham sá»‘: $L$: sá»‘ lÆ°á»£ng cÃ¡c block sub-layers trong transformer, $H$: kÃ­ch thÆ°á»›c cá»§a embedding vÃ©c tÆ¡ (hay cÃ²n gá»i lÃ  hidden size), $A$: Sá»‘ lÆ°á»£ng head trong multi-head layer, má»—i má»™t head sáº½ thá»±c hiá»‡n má»™t self-attention. TÃªn gá»i cá»§a 2 kiáº¿n trÃºc bao gá»“m:</p>

<ul>
  <li>
    <p>$\textbf{BERT}_{\textbf{BASE}}(L=12, H=768, A=12)$: Tá»•ng tham sá»‘ 110 triá»‡u.</p>
  </li>
  <li>
    <p>$\textbf{BERT}_{\textbf{LARGE}}(L=24, H=1024, A=16)$: Tá»•ng tham sá»‘ 340 triá»‡u.</p>
  </li>
</ul>

<p>NhÆ° váº­y á»Ÿ kiáº¿n trÃºc BERT Large chÃºng ta tÄƒng gáº¥p Ä‘Ã´i sá»‘ layer, tÄƒng kÃ­ch thÆ°á»›c hidden size cá»§a embedding vÃ©c tÆ¡ gáº¥p 1.33 láº§n vÃ  tÄƒng sá»‘ lÆ°á»£ng head trong multi-head layer gáº¥p 1.33 láº§n.</p>

<h1 id="4-thá»±c-hÃ nh-model-bert">4. Thá»±c hÃ nh model BERT</h1>
<h2 id="41-giá»›i-thiá»‡u-vá»-bÃ i-toÃ¡n">4.1. Giá»›i thiá»‡u vá» bÃ i toÃ¡n</h2>

<p>ChÃºng ta sáº½ cÃ¹ng xÃ¢y dá»±ng má»™t á»©ng dá»¥ng Question and Answering cÃ³ chá»©c nÄƒng há»i Ä‘Ã¡p.</p>

<p>Dá»¯ liá»‡u bao gá»“m:</p>

<p>Input: Má»™t cáº·p cÃ¢u &lt;Question, Paragraph&gt;, Question lÃ  cÃ¢u há»i vÃ  Paragraph lÃ  Ä‘oáº¡n vÄƒn báº£n chá»©a cÃ¢u tráº£ lá»i cho cÃ¢u há»i.</p>

<p>Output: CÃ¢u tráº£ lá»i Ä‘Æ°á»£c trÃ­ch suáº¥t tá»« Paragraph.</p>

<p>ÄÃ¢y lÃ  má»™t á»©ng dá»¥ng khÃ¡ thÃº vá»¥ mÃ  tÃ´i Ä‘Ã£ compile thÃ nh cÃ´ng trÃªn thiáº¿t bá»‹ android. CÃ¡c báº¡n cÃ³ thá»ƒ download á»©ng dá»¥ng vá» vÃ  cháº¡y thá»­ nghiá»‡m <a href="https://www.facebook.com/TowardDataScience/videos/201232064499053/">BERT - Tensorflow Lite - Khanh Blog</a>.</p>

<p>Äá»ƒ thá»±c hiá»‡n tÃ¡c vá»¥ nÃ y tÃ´i sáº½ sá»­ dá»¥ng pretrain model tá»« package transformer. ChÃºng ta cÃ³ thá»ƒ cÃ i thÃ´ng qua cÃ¢u lá»‡nh bÃªn dÆ°á»›i.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>!pip install transformers
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="rouge-code"><pre>Collecting transformers
[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)
[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665kB 2.8MB/s 
[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)
Collecting tokenizers==0.7.0
[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)
[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 14.6MB/s 
[?25hRequirement already satisfied: dataclasses; python_version &lt; "3.7" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)
Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)
Collecting sacremoses
[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)
[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 28.9MB/s 
[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)
Collecting sentencepiece
[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)
[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 44.5MB/s 
[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)
Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)
Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (1.12.0)
Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (7.1.2)
Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (0.15.1)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (1.24.3)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (2.9)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (2020.4.5.1)
Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (3.0.4)
Building wheels for collected packages: sacremoses
  Building wheel for sacremoses (setup.py) ... [?25l[?25hdone
  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=6a91001f4e422c7561a79667a70494bc9d94fdf57f29570ca9ac2f2f582a21ec
  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45
Successfully built sacremoses
Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers
Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="42-xÃ¢y-dá»±ng-má»™t-á»©ng-dá»¥ng-question-and-answering">4.2. XÃ¢y dá»±ng má»™t á»©ng dá»¥ng Question and Answering</h2>

<p>CÃ¡c bÆ°á»›c dá»¯ liá»‡u:</p>

<ul>
  <li>
    <p><strong>Tokenize</strong>: Táº¡o chuá»—i token lÃ  concatenate cá»§a cáº·p cÃ¢u <code class="highlighter-rouge">&lt;Question, Paragraph&gt;</code>, thÃªm cÃ¡c token <code class="highlighter-rouge">[CLS]</code> Ä‘Ã¡nh dáº¥u vá»‹ trÃ­ báº¯t Ä‘áº§u cÃ¢u <code class="highlighter-rouge">Question</code> vÃ  <code class="highlighter-rouge">[SEP]</code> Ä‘Ã¡nh dáº¥u vá»‹ trÃ­ káº¿t thÃºc cÃ¢u. Sau Ä‘Ã³ Tokenize toÃ n bá»™ cáº·p cÃ¢u <code class="highlighter-rouge">&lt;Question, Paragraph&gt;</code> thÃ nh chuá»—i index tá»« tá»« Ä‘iá»ƒn.</p>
  </li>
  <li>
    <p><strong>Set Segment IDs</strong>: Táº¡o vÃ©c tÆ¡ segment cho cáº·p cÃ¢u <code class="highlighter-rouge">Question</code> vÃ  <code class="highlighter-rouge">Paragraph</code>. Trong Ä‘Ã³ index 0 Ä‘Ã¡nh dáº¥u cÃ¡c vá»‹ trÃ­ thuá»™c cÃ¢u A vÃ  index 1 Ä‘Ã¡nh dáº¥u cÃ¡c vá»‹ trÃ­ thuá»™c cÃ¢u B.</p>
  </li>
  <li>
    <p><strong>Evaluate</strong>: Khá»Ÿi táº¡o model tá»« pretrain model <code class="highlighter-rouge">bert-large-uncased-whole-word-masking-finetuned-squad</code>. VÃ  dá»± bÃ¡o cÃ¡c vá»‹ trÃ­ <code class="highlighter-rouge">start</code> vÃ  <code class="highlighter-rouge">end</code> náº±m trong chuá»—i token.</p>
  </li>
  <li>
    <p><strong>Reconstruct Answer</strong>:TrÃ­ch suáº¥t thÃ´ng tin cÃ¢u tráº£ lá»i.</p>
  </li>
</ul>

<p>Source code cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c tham kháº£o táº¡i <a href="https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT">Question answering with fine tuned BERT</a></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
</pre></td><td class="rouge-code"><pre><span class="k">from</span> <span class="n">transformers</span> <span class="n">import</span> <span class="n">BertTokenizer</span>
<span class="k">from</span> <span class="n">transformers</span> <span class="n">import</span> <span class="n">BertForQuestionAnswering</span>
<span class="n">import</span> <span class="n">torch</span>
<span class="p">#</span> <span class="n">Initialize</span> <span class="n">tokenizer</span> <span class="n">for</span> <span class="n">corpus</span> <span class="k">of</span> <span class="n">bert</span><span class="p">-</span><span class="n">large</span><span class="p">-</span><span class="n">uncased</span>
<span class="n">tokenizer</span> <span class="p">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-large-uncased-whole-word-masking-finetuned-squad'</span><span class="p">)</span>

<span class="p">#</span> <span class="n">Initialize</span> <span class="k">model</span> <span class="n">BertForQuestionAnswering</span> <span class="n">for</span> <span class="n">bert</span><span class="p">-</span><span class="n">large</span><span class="p">-</span><span class="n">uncased</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">BertForQuestionAnswering</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-large-uncased-whole-word-masking-finetuned-squad'</span><span class="p">)</span>

<span class="n">def</span> <span class="n">answer_question</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">answer_text</span><span class="p">):</span>
    <span class="s1">'''
    Láº¥y input lÃ  chuá»—i string cá»§a cÃ¢u question vÃ  answer_text chá»©a ná»™i dung cÃ¢u tráº£ lá»i cá»§a cÃ¢u question.
    XÃ¡c Ä‘á»‹nh tá»« trong answer_text lÃ  cÃ¢u tráº£ lá»i vÃ  in ra.
    '''</span>
    <span class="p">#</span> <span class="p">========</span> <span class="n">Tokenize</span> <span class="p">========</span>
    <span class="p">#</span> <span class="err">Ã</span><span class="n">p</span> <span class="n">d</span><span class="err">á»¥</span><span class="n">ng</span> <span class="n">tokenizer</span> <span class="n">cho</span> <span class="n">c</span><span class="err">áº·</span><span class="n">p</span> <span class="n">c</span><span class="err">Ã¢</span><span class="n">u</span> <span class="p">&lt;</span><span class="n">question</span><span class="p">,</span> <span class="n">answer_text</span><span class="p">&gt;.</span> <span class="n">input_ids</span> <span class="n">l</span><span class="err">Ã </span> <span class="n">concatenate</span> <span class="n">indice</span> <span class="n">c</span><span class="err">á»§</span><span class="n">a</span> <span class="n">c</span><span class="err">áº£</span> <span class="m">2</span> <span class="n">c</span><span class="err">Ã¢</span><span class="n">u</span> <span class="n">sau</span> <span class="n">khi</span> <span class="err">Ä‘Ã£</span> <span class="n">th</span><span class="err">Ãª</span><span class="n">m</span> <span class="n">c</span><span class="err">Ã¡</span><span class="n">c</span> <span class="n">token</span> <span class="n">CLS</span> <span class="n">v</span><span class="err">Ã </span> <span class="n">SEP</span> <span class="n">nh</span><span class="err">Æ°</span> <span class="n">m</span><span class="err">Ã´</span> <span class="n">t</span><span class="err">áº£</span> <span class="n">trong</span> <span class="n">t</span><span class="err">Ã¡</span><span class="n">c</span> <span class="n">v</span><span class="err">á»¥</span> <span class="n">Question</span> <span class="k">and</span> <span class="n">Answering</span><span class="p">.</span>
    <span class="n">input_ids</span> <span class="p">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">answer_text</span><span class="p">)</span>

    <span class="p">#</span> <span class="p">========</span> <span class="k">Set</span> <span class="n">Segment</span> <span class="n">IDs</span> <span class="p">========</span>
    <span class="p">#</span> <span class="n">X</span><span class="err">Ã¡</span><span class="n">c</span> <span class="err">Ä‘á»‹</span><span class="n">nh</span> <span class="n">v</span><span class="err">á»‹</span> <span class="n">tr</span><span class="err">Ã­</span> <span class="err">Ä‘áº§</span><span class="n">u</span> <span class="n">ti</span><span class="err">Ãª</span><span class="n">n</span> <span class="n">ch</span><span class="err">á»©</span><span class="n">a</span> <span class="n">token</span> <span class="p">[</span><span class="n">SEP</span><span class="p">]</span> <span class="n">trong</span> <span class="n">c</span><span class="err">Ã¢</span><span class="n">u</span><span class="p">.</span>
    <span class="n">sep_index</span> <span class="p">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">index</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">sep_token_id</span><span class="p">)</span>

    <span class="p">#</span> <span class="n">T</span><span class="err">áº¡</span><span class="n">o</span> <span class="n">segment</span> <span class="n">index</span> <span class="err">Ä‘Ã¡</span><span class="n">nh</span> <span class="n">d</span><span class="err">áº¥</span><span class="n">u</span> <span class="n">c</span><span class="err">Ã¡</span><span class="n">c</span> <span class="n">v</span><span class="err">á»‹</span> <span class="n">tr</span><span class="err">Ã­</span> <span class="n">t</span><span class="err">á»«</span> <span class="n">thu</span><span class="err">á»™</span><span class="n">c</span> <span class="n">question</span> <span class="p">(</span><span class="n">gi</span><span class="err">Ã¡</span> <span class="n">tr</span><span class="err">á»‹</span> <span class="m">0</span><span class="p">)</span> <span class="n">v</span><span class="err">Ã </span> <span class="n">answer_text</span> <span class="p">(</span><span class="n">gi</span><span class="err">Ã¡</span> <span class="n">tr</span><span class="err">á»‹</span> <span class="m">1</span><span class="p">)</span>
    <span class="n">num_seg_a</span> <span class="p">=</span> <span class="n">sep_index</span> <span class="p">+</span> <span class="m">1</span>
    <span class="n">num_seg_b</span> <span class="p">=</span> <span class="n">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="p">-</span> <span class="n">num_seg_a</span>
    <span class="n">segment_ids</span> <span class="p">=</span> <span class="p">[</span><span class="m">0</span><span class="p">]*</span><span class="n">num_seg_a</span> <span class="p">+</span> <span class="p">[</span><span class="m">1</span><span class="p">]*</span><span class="n">num_seg_b</span>

    <span class="p">#</span> <span class="n">Ki</span><span class="err">á»ƒ</span><span class="n">m</span> <span class="n">tra</span> <span class="err">Ä‘á»™</span> <span class="n">d</span><span class="err">Ã </span><span class="n">i</span> <span class="n">segment_ids</span> <span class="n">ph</span><span class="err">áº£</span><span class="n">i</span> <span class="n">b</span><span class="err">áº±</span><span class="n">ng</span> <span class="n">input_ids</span>
    <span class="nb">assert</span> <span class="n">len</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">)</span> <span class="p">==</span> <span class="n">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="p">#</span> <span class="p">========</span> <span class="n">Evaluate</span> <span class="p">========</span>
    <span class="p">#</span> <span class="n">D</span><span class="err">á»±</span> <span class="n">b</span><span class="err">Ã¡</span><span class="n">o</span> <span class="n">ph</span><span class="err">Ã¢</span><span class="n">n</span> <span class="n">ph</span><span class="err">á»‘</span><span class="n">i</span> <span class="n">x</span><span class="err">Ã¡</span><span class="n">c</span> <span class="n">su</span><span class="err">áº¥</span><span class="n">t</span> <span class="n">c</span><span class="err">á»§</span><span class="n">a</span> <span class="n">v</span><span class="err">á»‹</span> <span class="n">tr</span><span class="err">Ã­</span> <span class="n">c</span><span class="err">á»§</span><span class="n">a</span> <span class="n">t</span><span class="err">á»«</span> <span class="n">start</span> <span class="n">v</span><span class="err">Ã </span> <span class="n">t</span><span class="err">á»«</span> <span class="k">end</span> <span class="n">trong</span> <span class="n">chu</span><span class="err">á»—</span><span class="n">i</span> <span class="n">concatenate</span> <span class="p">&lt;</span><span class="n">question</span><span class="p">,</span> <span class="n">answer_text</span><span class="p">&gt;</span> <span class="n">m</span><span class="err">Ã </span> <span class="n">ch</span><span class="err">á»©</span><span class="n">a</span> <span class="n">k</span><span class="err">áº¿</span><span class="n">t</span> <span class="n">qu</span><span class="err">áº£</span> <span class="n">cho</span> <span class="n">c</span><span class="err">Ã¢</span><span class="n">u</span> <span class="n">tr</span><span class="err">áº£</span> <span class="n">l</span><span class="err">á»</span><span class="n">i</span><span class="p">.</span>
    <span class="n">start_scores</span><span class="p">,</span> <span class="n">end_scores</span> <span class="p">=</span> <span class="k">model</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">input_ids</span><span class="p">]),</span> <span class="p">#</span> <span class="n">chu</span><span class="err">á»—</span><span class="n">i</span> <span class="n">index</span> <span class="n">bi</span><span class="err">á»ƒ</span><span class="n">u</span> <span class="n">th</span><span class="err">á»‹</span> <span class="n">cho</span> <span class="n">inputs</span><span class="p">.</span>
                                    <span class="n">token_type_ids</span><span class="p">=</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">segment_ids</span><span class="p">]))</span> <span class="p">#</span> <span class="n">chu</span><span class="err">á»—</span><span class="n">i</span> <span class="n">index</span> <span class="n">th</span><span class="err">Ã </span><span class="n">nh</span> <span class="n">ph</span><span class="err">áº§</span><span class="n">n</span> <span class="n">segment</span> <span class="n">c</span><span class="err">Ã¢</span><span class="n">u</span> <span class="err">Ä‘á»ƒ</span> <span class="n">ph</span><span class="err">Ã¢</span><span class="n">n</span> <span class="n">bi</span><span class="err">á»‡</span><span class="n">t</span> <span class="n">gi</span><span class="err">á»¯</span><span class="n">a</span> <span class="n">c</span><span class="err">Ã¢</span><span class="n">u</span> <span class="n">question</span> <span class="n">v</span><span class="err">Ã </span> <span class="n">c</span><span class="err">Ã¢</span><span class="n">u</span> <span class="n">answer_text</span>

    <span class="p">#</span> <span class="p">========</span> <span class="n">Reconstruct</span> <span class="n">Answer</span> <span class="p">========</span>
    <span class="p">#</span> <span class="n">T</span><span class="err">Ã¬</span><span class="n">m</span> <span class="n">ra</span> <span class="n">v</span><span class="err">á»‹</span> <span class="n">tr</span><span class="err">Ã­</span> <span class="n">start</span><span class="p">,</span> <span class="k">end</span> <span class="n">v</span><span class="err">á»›</span><span class="n">i</span> <span class="n">score</span> <span class="n">l</span><span class="err">Ã </span> <span class="n">cao</span> <span class="n">nh</span><span class="err">áº¥</span><span class="n">t</span>
    <span class="n">answer_start</span> <span class="p">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">start_scores</span><span class="p">)</span>
    <span class="n">answer_end</span> <span class="p">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">end_scores</span><span class="p">)</span>

    <span class="p">#</span> <span class="n">Chuy</span><span class="err">á»ƒ</span><span class="n">n</span> <span class="n">ng</span><span class="err">Æ°á»£</span><span class="n">c</span> <span class="n">t</span><span class="err">á»«</span> <span class="n">input_ids</span> <span class="n">sang</span> <span class="k">list</span> <span class="n">tokens</span>
    <span class="n">tokens</span> <span class="p">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="p">#</span> <span class="n">Token</span> <span class="err">Ä‘áº§</span><span class="n">u</span> <span class="n">ti</span><span class="err">Ãª</span><span class="n">n</span> <span class="n">c</span><span class="err">á»§</span><span class="n">a</span> <span class="n">c</span><span class="err">Ã¢</span><span class="n">u</span> <span class="n">tr</span><span class="err">áº£</span> <span class="n">l</span><span class="err">á»</span><span class="n">i</span>
    <span class="n">answer</span> <span class="p">=</span> <span class="n">tokens</span><span class="p">[</span><span class="n">answer_start</span><span class="p">]</span>

    <span class="p">#</span> <span class="n">L</span><span class="err">á»±</span><span class="n">a</span> <span class="n">ch</span><span class="err">á»</span><span class="n">n</span> <span class="n">c</span><span class="err">Ã¡</span><span class="n">c</span> <span class="n">th</span><span class="err">Ã </span><span class="n">nh</span> <span class="n">ph</span><span class="err">áº§</span><span class="n">n</span> <span class="n">c</span><span class="err">Ã²</span><span class="n">n</span> <span class="n">l</span><span class="err">áº¡</span><span class="n">i</span> <span class="n">c</span><span class="err">á»§</span><span class="n">a</span> <span class="n">c</span><span class="err">Ã¢</span><span class="n">u</span> <span class="n">tr</span><span class="err">áº£</span> <span class="n">l</span><span class="err">á»</span><span class="n">i</span> <span class="n">v</span><span class="err">Ã </span> <span class="n">join</span> <span class="n">ch</span><span class="err">Ãº</span><span class="n">ng</span> <span class="n">v</span><span class="err">á»›</span><span class="n">i</span> <span class="n">whitespace</span><span class="p">.</span>
    <span class="n">for</span> <span class="n">i</span> <span class="k">in</span> <span class="k">range</span><span class="p">(</span><span class="n">answer_start</span> <span class="p">+</span> <span class="m">1</span><span class="p">,</span> <span class="n">answer_end</span> <span class="p">+</span> <span class="m">1</span><span class="p">):</span>
        
        <span class="p">#</span> <span class="n">N</span><span class="err">áº¿</span><span class="n">u</span> <span class="n">token</span> <span class="n">l</span><span class="err">Ã </span> <span class="n">m</span><span class="err">á»™</span><span class="n">t</span> <span class="n">subword</span> <span class="n">token</span> <span class="p">(</span><span class="n">c</span><span class="err">Ã³</span> <span class="n">d</span><span class="err">áº¥</span><span class="n">u</span> <span class="p">##</span> <span class="err">á»Ÿ</span> <span class="err">Ä‘áº§</span><span class="n">u</span><span class="p">)</span> <span class="n">th</span><span class="err">Ã¬</span> <span class="n">combine</span> <span class="n">v</span><span class="err">Ã </span><span class="n">o</span> <span class="n">answer</span> <span class="n">b</span><span class="err">áº±</span><span class="n">ng</span> <span class="n">token</span> <span class="n">g</span><span class="err">á»‘</span><span class="n">c</span> <span class="p">(</span><span class="n">lo</span><span class="err">áº¡</span><span class="n">i</span> <span class="n">b</span><span class="err">á»</span> <span class="n">d</span><span class="err">áº¥</span><span class="n">u</span> <span class="p">##).</span>
        <span class="k">if</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="m">0</span><span class="p">:</span><span class="m">2</span><span class="p">]</span> <span class="p">==</span> <span class="s1">'##'</span><span class="p">:</span>
            <span class="n">answer</span> <span class="p">+=</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="m">2</span><span class="p">:]</span>
        
        <span class="p">#</span> <span class="n">N</span><span class="err">áº¿</span><span class="n">u</span> <span class="n">tr</span><span class="err">Ã¡</span><span class="n">i</span> <span class="n">l</span><span class="err">áº¡</span><span class="n">i</span> <span class="n">th</span><span class="err">Ã¬</span> <span class="n">combine</span> <span class="n">tr</span><span class="err">á»±</span><span class="n">c</span> <span class="n">ti</span><span class="err">áº¿</span><span class="n">p</span> <span class="n">v</span><span class="err">Ã </span><span class="n">o</span> <span class="n">answer</span><span class="p">.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">answer</span> <span class="p">+=</span> <span class="s1">' '</span> <span class="p">+</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">print</span><span class="p">(</span><span class="s1">'Question: "'</span> <span class="p">+</span> <span class="n">question</span> <span class="p">+</span> <span class="s1">'"'</span><span class="p">)</span>
    <span class="n">print</span><span class="p">(</span><span class="s1">'Answer: "'</span> <span class="p">+</span> <span class="n">answer</span> <span class="p">+</span> <span class="s1">'"'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Thá»­ nghiá»‡m káº¿t quáº£ cá»§a mÃ´ hÃ¬nh trÃªn má»™t vÃ i cáº·p cÃ¢u <code class="highlighter-rouge">&lt;Question, Paragraph&gt;</code>.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre>question = "what is my dog name?"
paragraph = "I have a dog. It's name is Ricky. I get it at my 15th birthday, when it was a puppy."

answer_question(question, paragraph)
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>Question: "what is my dog name?"
Answer: "ricky"
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Thá»­ nghiá»‡m má»™t vÄƒn báº£n khÃ¡c dÃ i hÆ¡n. TÃ´i sáº½ láº¥y má»™t Ä‘oáº¡n vÄƒn mÃ´ táº£ tiá»ƒu sá»­ cá»§a Ã´ng vua toÃ¡n há»c <code class="highlighter-rouge">Euler</code> vÃ  há»i thuáº­t toÃ¡n ngÃ y sinh cá»§a Ã´ng áº¥y. CÃ¡c báº¡n hÃ£y xem káº¿t quáº£ nhÃ©.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre>question = "when Leonhard Euler was born?"
paragraph = "Leonhard Euler: 15 April 1707 â€“ 18 September 1783 was a Swiss mathematician, \
physicist, astronomer, geographer, logician and engineer who made important and influential discoveries in many branches of mathematics, \
such as infinitesimal calculus and graph theory, \
while also making pioneering contributions to several branches such as topology and analytic number theory. \
He also introduced much of the modern mathematical terminology and notation, \
particularly for mathematical analysis, such as the notion of a mathematical function.[4] He is also known for his work in mechanics, fluid dynamics, optics, astronomy and music theory"

answer_question(question, paragraph)
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>Question: "when Leonhard Euler was born?"
Answer: "15 april 1707"
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Ta cÃ³ thá»ƒ tháº¥y káº¿t quáº£ lÃ  chÃ­nh xÃ¡c.</p>

<p>Viá»‡c Ã¡p dá»¥ng pretrain model sáºµn cÃ³ trÃªn package transformer cho tÃ¡c vá»¥ <code class="highlighter-rouge">Question and Answering</code> lÃ  khÃ¡ dá»… dÃ ng. ChÃºng ta cÅ©ng cÃ³ thá»ƒ fine-tuning láº¡i cÃ¡c kiáº¿n trÃºc model question and answering cho dá»¯ liá»‡u Tiáº¿ng Viá»‡t Ä‘á»ƒ táº¡o ra cÃ¡c á»©ng dá»¥ng há»i Ä‘Ã¡p cho riÃªng mÃ¬nh. Äá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c Ä‘iá»u Ä‘Ã³ Ä‘Ã²i há»i pháº£i náº¯m vá»¯ng kiáº¿n trÃºc cá»§a model BERT Ä‘Æ°á»£c trÃ¬nh bÃ y trong bÃ i viáº¿t nÃ y. CÃ³ láº½ á»Ÿ má»™t bÃ i sau tÃ´i sáº½ hÆ°á»›ng dáº«n cÃ¡c báº¡n thá»±c hÃ nh Ä‘iá»u nÃ y.</p>

<h1 id="5-tá»•ng-káº¿t">5. Tá»•ng káº¿t</h1>

<p>NhÆ° váº­y qua bÃ i nÃ y tÃ´i Ä‘Ã£ hÆ°á»›ng dáº«n cÃ¡c báº¡n kiáº¿n trÃºc tá»•ng quÃ¡t cá»§a model BERT vÃ  cÃ¡ch thá»©c Ã¡p dá»¥ng model BERT vÃ o trong cÃ¡c tÃ¡c vá»¥ down stream task trong NLP nhÆ° Masked ML, Next Sentence Prediction vÃ  thá»±c hÃ nh xÃ¢y dá»±ng má»™t á»©ng dá»¥ng Question and Answering ngay trÃªn pretrain model cá»§a transformer package.</p>

<p>CÃ¡c kiáº¿n trÃºc biáº¿n thá»ƒ má»›i cá»§a BERT hiá»‡n táº¡i váº«n Ä‘ang Ä‘Æ°á»£c nghiÃªn cá»©u vÃ  tiáº¿p tá»¥c phÃ¡t triá»ƒn nhÆ° <a href="https://huggingface.co/transformers/model_doc/roberta.html">ROBERTA</a>, <a href="https://huggingface.co/transformers/model_doc/albert.html">ALBERT</a>, <a href="https://huggingface.co/transformers/model_doc/camembert.html">CAMEBERT</a>, <a href="https://huggingface.co/transformers/model_doc/xlmroberta.html">XLMROBERTA</a>, â€¦</p>

<p>NgÃ y cÃ ng cÃ³ nhiá»u cÃ¡c pretrain model trÃªn BERT Ã¡p dá»¥ng cho nhiá»u ngÃ´n ngá»¯ khÃ¡c nhau trÃªn toÃ n tháº¿ giá»›i vÃ  táº¡o ra má»™t sá»± Ä‘á»™t phÃ¡ trong NLP. NgÃ´n ngá»¯ Tiáº¿ng Viá»‡t cá»§a chÃºng ta cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c VinAI nghiÃªn cá»©u vÃ  huáº¥n luyá»‡n pretrain model thÃ nh cÃ´ng. Báº¡n Ä‘á»c muá»‘n sá»­ dá»¥ng pretrain model nÃ y trong cÃ¡c tÃ¡c vá»¥ NLP cÃ³ thá»ƒ tham kháº£o thÃªm táº¡i <a href="https://github.com/VinAIResearch/PhoBERT">PhoBERT</a>.</p>

<h1 id="6-tÃ i-liá»‡u">6. TÃ i liá»‡u</h1>

<ol>
  <li>
    <p><a href="https://towardsdatascience.com/from-word-embeddings-to-pretrained-language-models-a-new-age-in-nlp-part-2-e9af9a0bdcd9">From word embeddings to Pretrained Language models</a></p>
  </li>
  <li>
    <p><a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">BERT explained state of the art language model for NLP</a></p>
  </li>
  <li>
    <p><a href="https://github.com/huggingface/transformers/">huggingface - transformer github package</a></p>
  </li>
  <li>
    <p><a href="https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT">question answering with a fine tuned BERT</a></p>
  </li>
  <li>
    <p><a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb">BERT fine-tuning with cloud</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
  </li>
  <li>
    <p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">OpenAI GPT - paper</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1801.06146">ULMFit paper</a></p>
  </li>
</ol>

<script data-ad-client="ca-pub-4263248182804679" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>

<script src="/js/toc.js"></script>
<script src="/js/btnTop.js"></script>
<script type="text/javascript">
$(document).ready(function() {
    $('#toc').toc();
});
</script>
				</div>
			</div>
			<div class="col-md-2 hidden-xs hidden-sm">
				<a  href="/">
					<img width="100%" style="padding-bottom: 3mm;" src="/assets/images/logo.jpg" /> </a>
				<br>
				<nav>
					<div class="header">Khanh's site</div>
					<li><a style="text-align: left; color: #074B80"  href="https://www.facebook.com/TowardDataScience">phamdinhkhanh blog</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://www.facebook.com/groups/3235479620010379/">phamdinhkhanh AICode forum</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://kaggle.com/phamdinhkhanh">phamdinhkhanh kaggle</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://rpubs.com/phamdinhkhanh">phamdinhkhanh rpub</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://github.com/phamdinhkhanh">phamdinhkhanh github</a></li>
					<br>
					<div class="header">other's site</div>
					<li><a style="text-align: left; color: #074B80"  href="https://www.facebook.com/groups/machinelearningcoban/">machine learning cÆ¡ báº£n facebook</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://forum.machinelearningcoban.com/">machine learning cÆ¡ báº£n forum</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://machinelearningmastery.com">machine learning mastery</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://viblo.asia">viblosia</a></li>
					<br>
					<div class="header">KhÃ³a há»c</div>
					<li><a style="text-align: left; color: #074B80"  href="http://web.stanford.edu/class/cs109/">XÃ¡c suáº¥t thá»‘ng kÃª(Probability): CS109</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://web.stanford.edu/class/cs246/">Bigdata: CS246</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://cs231n.stanford.edu/">Computer vision cÆ¡ báº£n: CS231N</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://web.stanford.edu/class/cs224n/">Natural Language Processing: CS224N</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://web.stanford.edu/class/cs224w/">KhoÃ¡ phÃ¢n tÃ­ch máº¡ng lÆ°á»›i (analysis of network): CS224W</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://web.stanford.edu/class/cs20si/">KhÃ³a há»c Tensorflow: CS20SI</a></li>
				</nav>
			</div>
		</div>
	</div>
	
</body>
</html>
