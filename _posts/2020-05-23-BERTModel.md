---
layout: post
author: phamdinhkhanh
title: BÃ i 36 - BERT model
---

# 1. Giá»›i thiá»‡u chung


## 1.1. Má»™t sá»‘ khÃ¡i niá»‡m

TrÆ°á»›c khi Ä‘i vÃ o bÃ i nÃ y, chÃºng ta cáº§n hiá»ƒu rÃµ má»™t sá»‘ khÃ¡i niá»‡m:

* **Nhiá»‡m vá»¥ phÃ­a sau (Downstream task)**: LÃ  nhá»¯ng tÃ¡c vá»¥ supervised-learning Ä‘Æ°á»£c cáº£i thiá»‡n dá»±a trÃªn nhá»¯ng pretrained model. VD: ChÃºng ta sá»­ dá»¥ng láº¡i cÃ¡c biá»ƒu diá»…n tá»« há»c Ä‘Æ°á»£c tá»« nhá»¯ng pretrained model trÃªn bá»™ vÄƒn báº£n lá»›n vÃ o má»™t tÃ¡c vá»¥ phÃ¢n tÃ­ch cáº£m xÃºc huáº¥n luyá»‡n trÃªn bá»™ vÄƒn báº£n cÃ³ **kÃ­ch thÆ°á»›c nhá» hÆ¡n**. Ãp dá»¥ng pretrain-embedding Ä‘Ã£ giÃºp cáº£i thiá»‡n mÃ´ hÃ¬nh. NhÆ° váº­y tÃ¡c vá»¥ sá»­ dá»¥ng pretrain-embedding Ä‘Æ°á»£c gá»i lÃ  downstream task.

* **Äiá»ƒm khÃ¡i quÃ¡t Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ hiá»ƒu ngÃ´n ngá»¯ (GLUE score benchmark)**: [GLUE score benchmark](https://gluebenchmark.com/) lÃ  má»™t táº­p há»£p cÃ¡c chá»‰ sá»‘ Ä‘Æ°á»£c xÃ¢y dá»±ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ khÃ¡i quÃ¡t má»©c Ä‘á»™ hiá»ƒu ngÃ´n ngá»¯ cá»§a cÃ¡c model NLP. CÃ¡c Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cÃ¡c bá»™ dá»¯ liá»‡u tiÃªu chuáº©n Ä‘Æ°á»£c qui Ä‘á»‹nh táº¡i cÃ¡c convention vá» phÃ¡t triá»ƒn vÃ  thÃºc Ä‘áº©y NLP. Má»—i bá»™ dá»¯ liá»‡u tÆ°Æ¡ng á»©ng vá»›i má»™t loáº¡i tÃ¡c NLP vá»¥ nhÆ°: PhÃ¢n tÃ­ch cáº£m xÃºc (Sentiment Analysis), há»i Ä‘Ã¡p (Question and Answering), dá»± bÃ¡o cÃ¢u tiáº¿p theo (NSP - Next Sentence Prediction), nháº­n diá»‡n thá»±c thá»ƒ trong cÃ¢u (NER - Name Entity Recognition), suy luáº­n ngÃ´n ngá»¯ tá»± nhiÃªn (NLI - Natural Languague Inference). Náº¿u báº¡n muá»‘n tÃ¬m hiá»ƒu thÃªm vá» cÃ¡ch tÃ­nh GLUE score vÃ  cÃ¡c bá»™ dá»¯ liá»‡u trong GLUE cÃ³ thá»ƒ Ä‘á»c thÃªm [tensorflow - glue](https://www.tensorflow.org/datasets/catalog/glue).

* **Quan há»‡ vÄƒn báº£n (Textual Entailment)**: LÃ  tÃ¡c vá»¥ Ä‘Ã¡nh giÃ¡ má»‘i quan há»‡ Ä‘á»‹nh hÆ°á»›ng giá»¯a 2 vÄƒn báº£n? NhÃ£n output cá»§a cÃ¡c cáº·p cÃ¢u Ä‘Æ°á»£c chia thÃ nh Ä‘á»‘i láº­p (contradiction), trung láº­p (neutral) hay cÃ³ quan há»‡ Ä‘i kÃ¨m (textual entailment). Cá»¥ thá»ƒ hÆ¡n, chÃºng ta cÃ³ cÃ¡c cÃ¢u:

A: HÃ´m nay trá»i mÆ°a.

B: TÃ´i mang Ã´ tá»›i trÆ°á»ng.

C: HÃ´m nay trá»i khÃ´ng mÆ°a.

D: HÃ´m nay lÃ  thá»© 3.

Khi Ä‘Ã³ (A, B) cÃ³ má»‘i quan há»‡ Ä‘i kÃ¨m. CÃ¡c cáº·p cÃ¢u (A, C) cÃ³ má»‘i quan há»‡ Ä‘á»•i láº­p vÃ  (A, D) lÃ  trung láº­p.

* **Suy luáº­n ngÃ´n ngá»¯ (Natural Language Inference)**: LÃ  cÃ¡c tÃ¡c vá»¥ suy luáº­n ngÃ´n ngá»¯ Ä‘Ã¡nh giÃ¡ má»‘i quan há»‡ giá»¯a cÃ¡c cáº·p cÃ¢u, cÅ©ng tÆ°Æ¡ng tá»± nhÆ° Textual Entailment.

* **PhÃ¢n tÃ­ch cáº£m xÃºc (Sentiment Analysis)**: PhÃ¢n loáº¡i cáº£m xÃºc vÄƒn báº£n thÃ nh 2 nhÃ£n tÃ­ch cá»±c (positive) vÃ  tiÃªu cá»±c (negative). ThÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c há»‡ thá»‘ng Ä‘Ã¡nh giÃ¡ bÃ¬nh luáº­n cá»§a ngÆ°á»i dÃ¹ng.

* **Há»i Ä‘Ã¡p (Question and Answering)**: LÃ  thuáº­t toÃ¡n há»i vÃ  Ä‘Ã¡p. Äáº§u vÃ o lÃ  má»™t cáº·p cÃ¢u (pair sequence) bao gá»“m: cÃ¢u há»i (question) cÃ³ chá»©c nÄƒng há»i vÃ  Ä‘oáº¡n vÄƒn báº£n (paragraph) chá»©a thÃ´ng tin tráº£ lá»i cho cÃ¢u há»i. Má»™t bá»™ dá»¯ liá»‡u chuáº©n náº±m trong GLUE dataset Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ tÃ¡c vá»¥ há»i vÃ  Ä‘Ã¡p lÃ  [SQuAD - Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/). ÄÃ¢y lÃ  má»™t bÃ i toÃ¡n khÃ¡ thÃº vá»‹, cÃ¡c báº¡n cÃ³ thá»ƒ xem thÃªm á»©ng dá»¥ng [Question and Answering - BERT model](https://www.facebook.com/TowardDataScience/videos/201232064499053/) mÃ  mÃ¬nh Ä‘Ã£ sharing.

* **Ngá»¯ cáº£nh (Contextual)**: LÃ  ngá»¯ cáº£nh cá»§a tá»«. Má»™t tá»« Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi má»™t cÃ¡ch phÃ¡t Ã¢m nhÆ°ng khi Ä‘Æ°á»£c Ä‘áº·t trong nhá»¯ng cÃ¢u khÃ¡c nhau thÃ¬ cÃ³ thá»ƒ mang ngá»¯ nghÄ©a khÃ¡c nhau. ngá»¯ cáº£nh cÃ³ thá»ƒ coi lÃ  mÃ´i trÆ°á»ng xung quanh tá»« Ä‘á»ƒ gÃ³p pháº§n Ä‘á»‹nh nghÄ©a tá»«. VD: 

A: TÃ´i `Ä‘á»“ng` Ã½ vá»›i Ã½ kiáº¿n cá»§a anh.

B: LÃ£o Háº¡c pháº£i kiáº¿m tá»«ng `Ä‘á»“ng` Ä‘á»ƒ nuÃ´i cáº­u VÃ ng.

ThÃ¬ tá»« `Ä‘á»“ng` trong cÃ¢u A vÃ  B cÃ³ Ã½ nghÄ©a khÃ¡c nhau. ChÃºng ta biáº¿t Ä‘iá»u nÃ y vÃ¬ dá»±a vÃ o ngá»¯ cáº£nh cá»§a tá»«.

* **Hiá»‡n Ä‘áº¡i nháº¥t (SOTA)**: state-of-art lÃ  nhá»¯ng phÆ°Æ¡ng phÃ¡p, ká»¹ thuáº­t tá»‘t nháº¥t mang láº¡i hiá»‡u quáº£ cao nháº¥t tá»« trÆ°á»›c Ä‘áº¿n nay.

* **MÃ´ hÃ¬nh biá»ƒu diá»…n mÃ£ hÃ³a 2 chiá»u dá»±a trÃªn biáº¿n Ä‘á»•i (BERT-Bidirectional Encoder Representation from Transformer)**: MÃ´ hÃ¬nh BERT. ÄÃ¢y lÃ  lá»›p mÃ´ hÃ¬nh SOTA trong nhiá»u tÃ¡c vá»¥ cá»§a `GLUE score benchmark`.

* **LTR model**: lÃ  mÃ´ hÃ¬nh há»c bá»‘i cáº£nh theo má»™t chiá»u duy nháº¥t tá»« trÃ¡i sang pháº£i. Cháº³ng háº¡n nhÆ° lá»›p cÃ¡c model RNN.

* **MLM (Masked Language Model)**: LÃ  mÃ´ hÃ¬nh mÃ  bá»‘i cáº£nh cá»§a tá»« Ä‘Æ°á»£c há»c tá»« cáº£ 2 phÃ­a bÃªn trÃ¡i vÃ  bÃªn pháº£i cÃ¹ng má»™t lÃºc tá»« nhá»¯ng bá»™ dá»¯ liá»‡u unsupervised text. Dá»¯ liá»‡u input sáº½ Ä‘Æ°á»£c masked (tá»©c thay báº±ng má»™t token MASK) má»™t cÃ¡ch ngáº«u nhiÃªn vá»›i tá»· lá»‡ tháº¥p. Huáº¥n luyá»‡n mÃ´ hÃ¬nh dá»± bÃ¡o tá»« Ä‘Æ°á»£c masked dá»±a trÃªn bá»‘i cáº£nh xung quanh lÃ  nhá»¯ng tá»« khÃ´ng Ä‘Æ°á»£c masked nháº±m tÃ¬m ra biá»ƒu diá»…n cá»§a tá»«.

## 1.2. LÃ½ do táº¡i sao mÃ¬nh viáº¿t vá» BERT?

Táº¡i thá»i Ä‘iá»ƒm mÃ¬nh viáº¿t vá» model BERT thÃ¬ BERT Ä‘Ã£ Ä‘Æ°á»£c ra Ä‘á»i khÃ¡ lÃ¢u. BERT lÃ  model biá»ƒu diá»…n ngÃ´n ngá»¯ Ä‘Æ°á»£c google giá»›i thiá»‡u vÃ o nÄƒm 2018. Táº¡i thá»i Ä‘iá»ƒm cÃ´ng bá»‘, BERT Ä‘Ã£ táº¡o ra má»™t sá»± rung Ä‘á»™ng trong cá»™ng Ä‘á»“ng NLP bá»Ÿi nhá»¯ng cáº£i tiáº¿n chÆ°a tá»«ng cÃ³ á»Ÿ nhá»¯ng model trÆ°á»›c Ä‘Ã³. Trong bÃ i bÃ¡o [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) cÃ¡c tÃ¡c giáº£ Ä‘Ã£ nÃªu ra nhá»¯ng cáº£i tiáº¿n cá»§a model BERT trong cÃ¡c tÃ¡c vá»¥:

* TÄƒng GLUE score (General Language Understanding Evaluation score), má»™t chá»‰ sá»‘ tá»•ng quÃ¡t Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ hiá»ƒu ngÃ´n ngá»¯ lÃªn `80.5%`.

* TÄƒng accuracy trÃªn bá»™ dá»¯ liá»‡u [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) Ä‘Ã¡nh giÃ¡ tÃ¡c vá»¥ quan há»‡ vÄƒn báº£n (text entailment) lÃªn 86.7%.

* TÄƒng accuracy F1 score trÃªn bá»™ dá»¯ liá»‡u [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) Ä‘Ã¡nh giÃ¡ tÃ¡c vá»¥ question and answering lÃªn 93.2%.

á» thá»i Ä‘iá»ƒm hiá»‡n táº¡i, BERT Ä‘Ã£ Ä‘Æ°á»£c á»©ng dá»¥ng cho Tiáº¿ng Viá»‡t. Báº¡n Ä‘á»c cÃ³ thá»ƒ tham kháº£o dá»± Ã¡n [PhoBERT](https://github.com/VinAIResearch/PhoBERT) cá»§a VinAI vá» huáº¥n luyá»‡n trÆ°á»›c biá»ƒu diá»…n tá»« (pre-train word embedding) sá»­ dá»¥ng model BERT. Má»™t sá»‘ báº¡n á»©ng dá»¥ng model PhoBERT vÃ o cÃ¡c tÃ¡c vá»¥ nhÆ° sentiment analysis vÃ  Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ cao nhÆ° [phÃ¢n loáº¡i cáº£m xÃºc bÃ¬nh luáº­n - KhÃ´i Nguyá»…n](https://github.com/suicao/PhoBert-Sentiment-Classification/).

BERT Ä‘Ã£ Ä‘Æ°á»£c ra Ä‘á»i lÃ¢u nhÆ° váº­y vÃ  cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c á»©ng dá»¥ng rá»™ng rÃ£i thÃ¬ táº¡i sao mÃ¬nh láº¡i viáº¿t vá» model nÃ y? ÄÃ³ lÃ  vÃ¬ BERT vÃ  cÃ¡c biáº¿n thá»ƒ mÃ´ hÃ¬nh cá»§a nÃ³ Ä‘ang lÃ  hot trend vÃ  sáº½ Ä‘á»‹nh hÆ°á»›ng cÃ¡c thuáº­t toÃ¡n NLP trong tÆ°Æ¡ng lai.

## 1.3. Ngá»¯ cáº£nh (Contextual) vÃ  vai trÃ² trong NLP

TrÆ°á»›c khi tÃ¬m hiá»ƒu cÃ¡c ká»¹ thuáº­t Ä‘Ã£ táº¡o ra Æ°u tháº¿ vÆ°á»£t trá»™i cho mÃ´ hÃ¬nh BERT. ChÃºng ta hÃ£y khÃ¡m phÃ¡ vai trÃ² cá»§a ngá»¯ cáº£nh trong NLP.

Báº£n cháº¥t cá»§a ngÃ´n ngá»¯ lÃ  Ã¢m thanh Ä‘Æ°á»£c phÃ¡t ra Ä‘á»ƒ diá»…n giáº£i dÃ²ng suy nghÄ© cá»§a con ngÆ°á»i. Trong giao tiáº¿p, cÃ¡c tá»« thÆ°á»ng khÃ´ng Ä‘á»©ng Ä‘á»™c láº­p mÃ  chÃºng sáº½ Ä‘i kÃ¨m vá»›i cÃ¡c tá»« khÃ¡c Ä‘á»ƒ liÃªn káº¿t máº¡ch láº¡c thÃ nh má»™t cÃ¢u. Hiá»‡u quáº£ biá»ƒu thá»‹ ná»™i dung vÃ  truyá»n Ä‘áº¡t Ã½ nghÄ©a sáº½ lá»›n hÆ¡n so vá»›i tá»«ng tá»« Ä‘á»©ng Ä‘á»™c láº­p.

Ngá»¯ cáº£nh trong cÃ¢u cÃ³ má»™t sá»± áº£nh hÆ°á»Ÿng ráº¥t lá»›n trong viá»‡c giáº£i thÃ­ch Ã½ nghÄ©a cá»§a tá»«. Hiá»ƒu Ä‘Æ°á»£c vai trÃ² máº¥u chá»‘t Ä‘Ã³, cÃ¡c thuáº­t toÃ¡n NLP SOTA Ä‘á»u cá»‘ gáº¯ng Ä‘Æ°a ngá»¯ cáº£nh vÃ o mÃ´ hÃ¬nh nháº±m táº¡o ra sá»± Ä‘á»™t phÃ¡ vÃ  cáº£i tiáº¿n vÃ  mÃ´ hÃ¬nh BERT cÅ©ng nhÆ° váº­y.

PhÃ¢n cáº¥p má»©c Ä‘á»™ phÃ¡t triá»ƒn cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p embedding tá»« trong NLP cÃ³ thá»ƒ bao gá»“m cÃ¡c nhÃ³m:

**Non-context (khÃ´ng bá»‘i cáº£nh)**: LÃ  cÃ¡c thuáº­t toÃ¡n khÃ´ng tá»“n táº¡i bá»‘i cáº£nh trong biá»ƒu diá»…n tá»«. ÄÃ³ lÃ  cÃ¡c thuáº­t toÃ¡n NLP Ä‘á»i Ä‘áº§u nhÆ° ` word2vec, GLoVe, fasttext`. ChÃºng ta chá»‰ cÃ³ duy nháº¥t má»™t biá»ƒu diá»…n vÃ©c tÆ¡ cho má»—i má»™t tá»« mÃ  khÃ´ng thay Ä‘á»•i theo bá»‘i cáº£nh. VD:

CÃ¢u A: `ÄÆ¡n vá»‹ tiá»n tá»‡ cá»§a Viá»‡t Nam lÃ  [Ä‘á»“ng]`

CÃ¢u B: `Vá»£ [Ä‘á»“ng] Ã½ vá»›i Ã½ kiáº¿n cá»§a chá»“ng lÃ  tÄƒng thÃªm má»—i thÃ¡ng 500k tiá»n tiÃªu váº·t`

ThÃ¬ tá»« Ä‘á»“ng sáº½ mang 2 Ã½ nghÄ©a khÃ¡c nhau nÃªn pháº£i cÃ³ hai biá»ƒu diá»…n tá»« riÃªng biá»‡t. CÃ¡c thuáº­t toÃ¡n non-context Ä‘Ã£ khÃ´ng Ä‘Ã¡p á»©ng Ä‘Æ°á»£c sá»± Ä‘a dáº¡ng vá» ngá»¯ nghÄ©a cá»§a tá»« trong NLP.

**Uni-directional (má»™t chiá»u)**: LÃ  cÃ¡c thuáº­t toÃ¡n Ä‘Ã£ báº¯t Ä‘áº§u xuáº¥t hiá»‡n bá»‘i cáº£nh cá»§a tá»«. CÃ¡c phÆ°Æ¡ng phÃ¡p nhÃºng tá»« base trÃªn RNN lÃ  nhá»¯ng phÆ°Æ¡ng phÃ¡p nhÃºng tá»« má»™t chiá»u. CÃ¡c káº¿t quáº£ biá»ƒu diá»…n tá»« Ä‘Ã£ cÃ³ bá»‘i cáº£nh nhÆ°ng chá»‰ Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi má»™t chiá»u tá»« trÃ¡i qua pháº£i hoáº·c tá»« pháº£i qua trÃ¡i. VD:

CÃ¢u C: HÃ´m nay tÃ´i mang 200 tá»· [gá»­i] á»Ÿ ngÃ¢n hÃ ng.

CÃ¢u D: HÃ´m nay tÃ´i mang 200 tá»· [gá»­i] ....

NhÆ° váº­y vÃ©c tÆ¡ biá»ƒu diá»…n cá»§a tá»« `gá»­i` Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh thÃ´ng qua cÃ¡c tá»« liá»n trÆ°á»›c vá»›i nÃ³. Náº¿u chá»‰ dá»±a vÃ o cÃ¡c tá»« liá»n trÆ°á»›c `HÃ´m nay tÃ´i mang 200 tá»·` thÃ¬ ta cÃ³ thá»ƒ nghÄ© tá»« phÃ¹ há»£p á»Ÿ vá»‹ trÃ­ hiá»‡n táº¡i lÃ  `cho vay, mua, thanh toÃ¡n,...`. 

VÃ­ dá»¥ Ä‘Æ¡n giáº£n trÃªn Ä‘Ã£ cho tháº¥y cÃ¡c thuáº­t toÃ¡n biá»ƒu diá»…n tá»« cÃ³ bá»‘i cáº£nh tuÃ¢n theo theo má»™t chiá»u sáº½ gáº·p háº¡n cháº¿ lá»›n trong biá»ƒu diá»…n tá»« hÆ¡n so vá»›i biá»ƒu diá»…n 2 chiá»u.

ELMo lÃ  má»™t vÃ­ dá»¥ cho phÆ°Æ¡ng phÃ¡p má»™t chiá»u. Máº·c dÃ¹ ELMo cÃ³ kiáº¿n trÃºc dá»±a trÃªn má»™t máº¡ng BiLSTM xem xÃ©t bá»‘i cáº£nh theo hai chiá»u tá»« trÃ¡i sang pháº£i vÃ  tá»« pháº£i sang trÃ¡i nhÆ°ng nhá»¯ng chiá»u nÃ y lÃ  Ä‘á»™c láº­p nhau nÃªn ta coi nhÆ° Ä‘Ã³ lÃ  biá»ƒu diá»…n má»™t chiá»u.

Thuáº­t toÃ¡n ELMo Ä‘Ã£ cáº£i tiáº¿n hÆ¡n so vá»›i word2vec vÃ  fasttext Ä‘Ã³ lÃ  táº¡o ra nghÄ©a cá»§a tá»« theo bá»‘i cáº£nh. Trong vÃ­ dá»¥ vá» tá»« `Ä‘á»“ng` thÃ¬ á»Ÿ má»—i cÃ¢u A vÃ  B chÃºng ta sáº½ cÃ³ má»™t biá»ƒu diá»…n tá»« khÃ¡c biá»‡t.

**Bi-directional (hai chiá»u)**: Ngá»¯ nghÄ©a cá»§a má»™t tá»« khÃ´ng chá»‰ Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi nhá»¯ng tá»« liá»n trÆ°á»›c mÃ  cÃ²n Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi toÃ n bá»™ cÃ¡c tá»« xung quanh. Luá»“ng giáº£i thÃ­ch tuÃ¢n theo **Ä‘á»“ng thá»i** tá»« trÃ¡i qua pháº£i vÃ  tá»« pháº£i qua trÃ¡i **cÃ¹ng má»™t lÃºc**. Äáº¡i diá»‡n cho cÃ¡c phÃ©p biá»ƒu diá»…n tá»« nÃ y lÃ  nhá»¯ng mÃ´ hÃ¬nh sá»­ dá»¥ng ká»¹ thuáº­t `transformer` mÃ  chÃºng ta sáº½ tÃ¬m hiá»ƒu bÃªn dÆ°á»›i. Gáº§n Ä‘Ã¢y, nhá»¯ng thuáº­t toÃ¡n NLP theo trÆ°á»ng phÃ¡i bidirectional nhÆ° [BERT](https://arxiv.org/abs/1810.04805), [ULMFit](https://arxiv.org/abs/1801.06146), [OpenAI GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)  Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng káº¿t quáº£ SOTA trÃªn háº§u háº¿t cÃ¡c tÃ¡c vá»¥ cá»§a `GLUE benchmark`.



## 1.4. Tiáº¿p cáº­n nÃ´ng vÃ  há»c sÃ¢u trong á»©ng dá»¥ng pre-training NLP



 


### 1.4.1. Tiáº¿p cáº­n nÃ´ng (shallow approach)

**Imagenet trong Computer Vision**

Trong xá»­ lÃ½ áº£nh chÃºng ta Ä‘á»u biáº¿t tá»›i nhá»¯ng pretrained models ná»•i tiáº¿ng trÃªn bá»™ dá»¯ liá»‡u Imagenet vá»›i 1000 classes. Nhá» sá»‘ lÆ°á»£ng classes lá»›n nÃªn háº§u háº¿t cÃ¡c nhÃ£n trong phÃ¢n loáº¡i áº£nh thÃ´ng thÆ°á»ng Ä‘á»u xuáº¥t hiá»‡n trong Imagenet vÃ  chÃºng ta cÃ³ thá»ƒ há»c chuyá»ƒn giao láº¡i cÃ¡c tÃ¡c vá»¥ xá»­ lÃ½ áº£nh ráº¥t nhanh vÃ  tiá»‡n lá»£i. ChÃºng ta cÅ©ng ká»³ vá»ng NLP cÃ³ má»™t tá»£p há»£p cÃ¡c pretrained models nhÆ° váº­y, tri thá»©c tá»« model Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c nguá»“n tÃ i nguyÃªn vÄƒn báº£n khÃ´ng nhÃ£n (unlabeled text) ráº¥t dá»“i dÃ o vÃ  sáºµn cÃ³.


**KhÃ³ khÄƒn há»c chuyá»ƒn giao trong NLP**

Tuy nhiÃªn trong NLP viá»‡c há»c chuyá»ƒn giao lÃ  khÃ´ng há» Ä‘Æ¡n giáº£n nhÆ° Computer Vision. Táº¡i sao váº­y?

CÃ¡c kiáº¿n trÃºc máº¡ng deep CNN cá»§a Computer Vision cho phÃ©p há»c chuyá»ƒn giao trÃªn Ä‘á»“ng thá»i cáº£ low-level vÃ  high-level features thÃ´ng qua viá»‡c táº­n dá»¥ng láº¡i cÃ¡c tham sá»‘ tá»« nhá»¯ng layers cá»§a mÃ´ hÃ¬nh pretrained.

NhÆ°ng trong NLP, cÃ¡c thuáº­t toÃ¡n cÅ© hÆ¡n nhÆ° `GLoVe, word2vec, fasttext` chá»‰ cho phÃ©p sá»­ dá»¥ng cÃ¡c biá»ƒu diá»…n vÃ©c tÆ¡ nhÃºng cá»§a tá»« lÃ  cÃ¡c low-level features nhÆ° lÃ  Ä‘áº§u vÃ o cho layer Ä‘áº§u tiÃªn cá»§a mÃ´ hÃ¬nh. CÃ¡c layers cÃ²n láº¡i giÃºp táº¡o ra high-level features thÃ¬ dÆ°á»ng nhÆ° Ä‘Æ°á»£c huáº¥n luyá»‡n láº¡i tá»« Ä‘áº§u.

NhÆ° váº­y chÃºng ta chá»‰ chuyá»ƒn giao Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng á»Ÿ má»©c Ä‘á»™ ráº¥t nÃ´ng nÃªn phÆ°Æ¡ng phÃ¡p nÃ y cÃ²n Ä‘Æ°á»£c gá»i lÃ  tiáº¿p cáº­n nÃ´ng (shallow approach). Viá»‡c tiáº¿p cáº­n vá»›i cÃ¡c layers sÃ¢u hÆ¡n lÃ  khÃ´ng thá»ƒ. Äiá»u nÃ y táº¡o ra má»™t háº¡n cháº¿ ráº¥t lá»›n Ä‘á»‘i vá»›i NLP so vá»›i Computer Vision trong viá»‡c há»c chuyá»ƒn giao. CÃ¡ch tiáº¿p cáº­n nÃ´ng trong há»c chuyá»ƒn giao cÃ²n Ä‘Æ°á»£c xem nhÆ° lÃ  **feature-based**. 

Khi Ã¡p dá»¥ng feature-based, chÃºng ta sáº½ táº­n dá»¥ng láº¡i cÃ¡c biá»ƒu diá»…n tá»« Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn nhá»¯ng kiáº¿n trÃºc mÃ´ hÃ¬nh cá»‘ Ä‘á»‹nh vÃ  nhá»¯ng bá»™ vÄƒn báº£n cÃ³ kÃ­ch thÆ°á»›c **ráº¥t lá»›n** Ä‘á»ƒ nÃ¢ng cao kháº£ nÄƒng biá»ƒu diá»…n tá»« trong khÃ´ng gian Ä‘a chiá»u. Má»™t sá»‘ pretrained feature-based báº¡n cÃ³ thá»ƒ Ã¡p dá»¥ng trong tiáº¿ng anh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n sáºµn Ä‘Ã³ lÃ  GloVe, [word2vec](https://wikipedia2vec.github.io/wikipedia2vec/pretrained/), [fasttext](https://fasttext.cc/docs/en/english-vectors.html), [ELMo](https://arxiv.org/abs/1802.05365).

### 1.4.2. Há»c sÃ¢u (deep-learning)

CÃ¡c mÃ´ hÃ¬nh NLP Ä‘á»™t phÃ¡ trong hai nÄƒm trá»Ÿ láº¡i Ä‘Ã¢y nhÆ° [BERT](https://arxiv.org/abs/1810.04805), [ELMo](https://arxiv.org/pdf/1802.05365), [ULMFit](https://arxiv.org/abs/1801.06146), [OpenAI GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) Ä‘Ã£ cho phÃ©p viá»‡c chuyá»ƒn giao layers trong NLP kháº£ thi hÆ¡n.

ChÃºng ta khÃ´ng chá»‰ há»c chuyá»ƒn giao Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng mÃ  cÃ²n chuyá»ƒn giao Ä‘Æ°á»£c kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh nhá» sá»‘ lÆ°á»£ng layers nhiá»u hÆ¡n, chiá»u sÃ¢u cá»§a mÃ´ hÃ¬nh sÃ¢u hÆ¡n trÆ°Æ¡c Ä‘Ã³.

CÃ¡c kiáº¿n trÃºc má»›i phÃ¢n cáº¥p theo level cÃ³ kháº£ nÄƒng chuyá»ƒn giao Ä‘Æ°á»£c nhá»¯ng cáº¥p Ä‘á»™ khÃ¡c nhau cá»§a Ä‘áº·c trÆ°ng tá»« low-level tá»›i high-level. Trong khi há»c nÃ´ng chá»‰ chuyá»ƒn giao Ä‘Æ°á»£c low-level táº¡i layer Ä‘áº§u tiÃªn. Táº¥t nhiÃªn low-level cÅ©ng Ä‘Ã³ng vai trÃ² quan trá»ng trong cÃ¡c tÃ¡c vá»¥ NLP. NhÆ°ng high-level lÃ  nhá»¯ng Ä‘áº·c trÆ°ng cÃ³ Ã½ nghÄ©a hÆ¡n vÃ¬ Ä‘Ã³ lÃ  nhá»¯ng Ä‘áº·c trÆ°ng Ä‘Ã£ Ä‘Æ°á»£c tinh luyá»‡n.

NgÆ°á»i ta ká»³ vá»ng ráº±ng `ULMFit, OpenAI GPT, BERT` sáº½ lÃ  nhá»¯ng mÃ´ hÃ¬nh pretrained giÃºp tiáº¿n gáº§n hÆ¡n tá»›i viá»‡c xÃ¢y dá»±ng má»™t lá»›p cÃ¡c pretrained models `ImageNet for NLP`. CÃ¡c báº¡n cÃ³ thá»ƒ xem thÃªm Ã½ tÆ°á»Ÿng vá» xÃ¢y dá»±ng [Imagenet for NLP](https://ruder.io/nlp-imagenet/).

Khi há»c chuyá»ƒn giao theo phÆ°Æ¡ng phÃ¡p há»c sÃ¢u chÃºng ta sáº½ táº­n dá»¥ng láº¡i kiáº¿n trÃºc tá»« mÃ´ hÃ¬nh pretrained vÃ  bá»• sung má»™t sá»‘ layers phÃ­a sau Ä‘á»ƒ phÃ¹ há»£p vá»›i nhiá»‡m vá»¥ huáº¥n luyá»‡n. CÃ¡c tham sá»‘ cá»§a cÃ¡c layers gá»‘c sáº½ Ä‘Æ°á»£c **fine-tunning** láº¡i. Chá»‰ má»™t sá»‘ Ã­t cÃ¡c tham sá»‘ á»Ÿ layers bá»• sung Ä‘Æ°á»£c huáº¥n luyá»‡n láº¡i tá»« Ä‘áº§u. Báº¡n Ä‘á»c cÃ³ thá»ƒ tÃ¬m hiá»ƒu thÃªm vá» fine-tuning táº¡i [BÃ i 33 - PhÆ°Æ¡ng phÃ¡p Transfer Learning](https://phamdinhkhanh.github.io/2020/04/15/TransferLearning.html).



## 2.1. PhÆ°Æ¡ng phÃ¡p transformer




### 2.1.1. Encoder vÃ  decoder trong BERT

TrÆ°á»›c khi hiá»ƒu vá» BERT chÃºng ta cÃ¹ng Ã´n láº¡i vá» ká»¹ thuáº­t transformer. MÃ¬nh Ä‘Ã£ diá»…n giáº£i ká»¹ thuáº­t nÃ y táº¡i [BÃ i 4 - Attention is all you need](https://phamdinhkhanh.github.io/2019/06/18/AttentionLayer.html). ÄÃ¢y lÃ m má»™t lá»›p mÃ´ hÃ¬nh seq2seq gá»“m 2 phrase encoder vÃ  decoder. MÃ´ hÃ¬nh hoÃ n toÃ n khÃ´ng sá»­ dá»¥ng cÃ¡c kiáº¿n trÃºc Recurrent Neural Network cá»§a RNN mÃ  chá»‰ sá»­ dá»¥ng cÃ¡c layers attention Ä‘á»ƒ embedding cÃ¡c tá»« trong cÃ¢u. Kiáº¿n trÃºc cá»¥ thá»ƒ cá»§a mÃ´ hÃ¬nh nhÆ° sau:

<img src="/assets/images/20200523_BERTModel/pic1.png" class="largepic" />

**HÃ¬nh 1:** SÆ¡ Ä‘á»“ kiáº¿n trÃºc transformer káº¿t há»£p vá»›i attention. Nguá»“n [attention is all you need](https://arxiv.org/abs/1706.03762).


MÃ´ hÃ¬nh sáº½ bao gá»“m 2 phase.

* **Encoder**: Bao gá»“m 6 layers liÃªn tiáº¿p nhau. Má»—i má»™t layer sáº½ bao gá»“m má»™t sub-layer lÃ  Multi-Head Attention káº¿t há»£p vá»›i fully-connected layer nhÆ° mÃ´ táº£ á»Ÿ nhÃ¡nh encoder bÃªn trÃ¡i cá»§a hÃ¬nh váº½. Káº¿t thÃºc quÃ¡ trÃ¬nh encoder ta thu Ä‘Æ°á»£c má»™t vector embedding output cho má»—i tá»«.

* **Decoder**: Kiáº¿n trÃºc cÅ©ng bao gá»“m cÃ¡c layers liÃªn tiáº¿p nhau. Má»—i má»™t layer cá»§a Decoder cÅ©ng cÃ³ cÃ¡c sub-layers gáº§n tÆ°Æ¡ng tá»± nhÆ° layer cá»§a Encoder nhÆ°ng bá»• sung thÃªm sub-layer Ä‘áº§u tiÃªn lÃ  `Masked Multi-Head Attention` cÃ³ tÃ¡c dá»¥ng loáº¡i bá» cÃ¡c tá»« trong tÆ°Æ¡ng lai khá»i quÃ¡ trÃ¬nh attention.

### 2.1.2. CÃ¡c tiáº¿n trÃ¬nh self-attention vÃ  encoder-decoder attention

Trong kiáº¿n trÃºc transformer chÃºng ta Ã¡p dá»¥ng 2 dáº¡ng attention khÃ¡c nhau táº¡i tá»«ng bÆ°á»›c huáº¥n luyá»‡n.

**self-attention**: ÄÆ°á»£c sá»­ dá»¥ng trong cÃ¹ng má»™t cÃ¢u input, táº¡i encoder hoáº·c táº¡i decoder. ÄÃ¢y chÃ­nh lÃ  attention Ä‘Æ°á»£c Ã¡p dá»¥ng táº¡i cÃ¡c Multi-Head Attention á»Ÿ Ä‘áº§u vÃ o cá»§a cáº£ 2 phase encoder vÃ  decoder.


<img src="/assets/images/20200523_BERTModel/pic2.png" class="largepic" />

**HÃ¬nh 2:** SÆ¡ Ä‘á»“ vá»‹ trÃ­ Ã¡p dá»¥ng self-attention trong kiáº¿n trÃºc transformer. CÃ¡c vÃ©c tÆ¡ embedding cá»§a cÃ¹ng má»™t chuá»—i encoder hoáº·c decoder tá»± liÃªn káº¿t vá»›i nhau Ä‘á»ƒ tÃ­nh toÃ¡n attention nhÆ° hÃ¬nh bÃªn pháº£i.


**Encoder-decoder attention**:

<img src="/assets/images/20200523_BERTModel/pic3.png" class="largepic" />

**HÃ¬nh 3:** BÃªn trÃ¡i lÃ  vá»‹ trÃ­ Ã¡p dá»¥ng encoder-decoder attention. BÃªn pháº£i lÃ  cÃ¡ch tÃ­nh trá»ng sá»‘ attention khi káº¿t há»£p má»—i vÃ©c tÆ¡ embedding á»Ÿ decoder vá»›i toÃ n bá»™ cÃ¡c vÃ©c tÆ¡ embedding á»Ÿ encoder.

Sá»Ÿ dÄ© Ä‘Æ°á»£c gá»i lÃ  encoder-decoder attention vÃ¬ Ä‘Ã¢y lÃ  kiáº¿n trÃºc attention tÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c vÃ©c tÆ¡ embedding cá»§a encoder vÃ  decoder. vÃ©c tÆ¡ context Ä‘Æ°á»£c tÃ­nh toÃ¡n trÃªn encoder Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh tÆ°Æ¡ng quan vá»›i vÃ©c tÆ¡ decoder nÃªn sáº½ cÃ³ Ã½ nghÄ©a giáº£i thÃ­ch bá»‘i cáº£nh cá»§a tá»« táº¡i vá»‹ trÃ­ time step decoder tÆ°Æ¡ng á»©ng. Sau khi káº¿t há»£p giá»¯a vÃ©c tÆ¡ context vÃ  vÃ©c tÆ¡ decoder ta sáº½ project tiáº¿p qua má»™t fully connected layer Ä‘á»ƒ tÃ­nh phÃ¢n phá»‘i xÃ¡c suáº¥t cho output.




Máº·c dÃ¹ cÃ³ kiáº¿n trÃºc chá»‰ gá»“m cÃ¡c biáº¿n Ä‘á»•i attention nhÆ°ng Transformer láº¡i cÃ³ káº¿t quáº£ ráº¥t tá»‘t trong cÃ¡c tÃ¡c vá»¥ NLP nhÆ° sentiment analysis vÃ  dá»‹ch mÃ¡y.

# 2. Giá»›i thiá»‡u vá» BERT

[BERT](https://arxiv.org/pdf/1810.04805.pdf) lÃ  viáº¿t táº¯t cá»§a cá»¥m tá»« `Bidirectional Encoder Representation from Transformer` cÃ³ nghÄ©a lÃ  mÃ´ hÃ¬nh biá»ƒu diá»…n tá»« theo 2 chiá»u á»©ng dá»¥ng ká»¹ thuáº­t `Transformer`. BERT Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ huáº¥n luyá»‡n trÆ°á»›c cÃ¡c biá»ƒu diá»…n tá»« (pre-train word embedding). Äiá»ƒm Ä‘áº·c biá»‡t á»Ÿ BERT Ä‘Ã³ lÃ  nÃ³ cÃ³ thá»ƒ Ä‘iá»u hÃ²a cÃ¢n báº±ng bá»‘i cáº£nh theo cáº£ 2 chiá»u trÃ¡i vÃ  pháº£i. 

CÆ¡ cháº¿ attention cá»§a Transformer sáº½ truyá»n toÃ n bá»™ cÃ¡c tá»« trong cÃ¢u vÄƒn Ä‘á»“ng thá»i vÃ o mÃ´ hÃ¬nh má»™t lÃºc mÃ  khÃ´ng cáº§n quan tÃ¢m Ä‘áº¿n chiá»u cá»§a cÃ¢u. Do Ä‘Ã³ Transformer Ä‘Æ°á»£c xem nhÆ° lÃ  huáº¥n luyá»‡n hai chiá»u (bidirectional) máº·c dÃ¹ trÃªn thá»±c táº¿ chÃ­nh xÃ¡c hÆ¡n chÃºng ta cÃ³ thá»ƒ nÃ³i ráº±ng Ä‘Ã³ lÃ  huáº¥n luyá»‡n khÃ´ng chiá»u (non-directional). Äáº·c Ä‘iá»ƒm nÃ y cho phÃ©p mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c bá»‘i cáº£nh cá»§a tá»« dá»±a trÃªn toÃ n bá»™ cÃ¡c tá»« xung quanh nÃ³ bao gá»“m cáº£ tá»« bÃªn trÃ¡i vÃ  tá»« bÃªn pháº£i.

## 2.1. Fine-tuning model BERT

Má»™t Ä‘iá»ƒm Ä‘áº·c biá»‡t á»Ÿ BERT mÃ  cÃ¡c model embedding trÆ°á»›c Ä‘Ã¢y chÆ°a tá»«ng cÃ³ Ä‘Ã³ lÃ  káº¿t quáº£ huáº¥n luyá»‡n cÃ³ thá»ƒ fine-tuning Ä‘Æ°á»£c. ChÃºng ta sáº½ thÃªm vÃ o kiáº¿n trÃºc model má»™t output layer Ä‘á»ƒ tÃ¹y biáº¿n theo tÃ¡c vá»¥ huáº¥n luyá»‡n.

<img src="/assets/images/20200523_BERTModel/pic4.png" class="largepic" />

**HÃ¬nh 4:** ToÃ n bá»™ tiáº¿n trÃ¬nh pre-training vÃ  fine-tuning cá»§a BERT. Má»™t kiáº¿n trÃºc tÆ°Æ¡ng tá»± Ä‘Æ°á»£c sá»­ dá»¥ng cho cáº£ pretrain-model vÃ  fine-tuning model. ChÃºng ta sá»­ dá»¥ng cÃ¹ng má»™t tham sá»‘ pretrain Ä‘á»ƒ khá»Ÿi táº¡o mÃ´ hÃ¬nh cho cÃ¡c tÃ¡c vá»¥ down stream khÃ¡c nhau. Trong suá»‘t quÃ¡ trÃ¬nh fine-tuning thÃ¬ toÃ n bá»™ cÃ¡c tham sá»‘ cá»§a layers há»c chuyá»ƒn giao sáº½ Ä‘Æ°á»£c fine-tune. Äá»‘i vá»›i cÃ¡c tÃ¡c vá»¥ sá»­ dá»¥ng input lÃ  má»™t cáº·p sequence (pair-sequence) vÃ­ dá»¥ nhÆ° `question and answering` thÃ¬ ta sáº½ thÃªm token khá»Ÿi táº¡o lÃ  `[CLS]` á»Ÿ Ä‘áº§u cÃ¢u, token `[SEP]` á»Ÿ giá»¯a Ä‘á»ƒ ngÄƒn cÃ¡ch 2 cÃ¢u.

Tiáº¿n trÃ¬nh Ã¡p dá»¥ng fine-tuning sáº½ nhÆ° sau:

* **BÆ°á»›c 1**: Embedding toÃ n bá»™ cÃ¡c token cá»§a cáº·p cÃ¢u báº±ng cÃ¡c vÃ©c tÆ¡ nhÃºng tá»« pretrain model. CÃ¡c token embedding bao gá»“m cáº£ 2 token lÃ  `[CLS]` vÃ  `[SEP]` Ä‘á»ƒ Ä‘Ã¡nh dáº¥u vá»‹ trÃ­ báº¯t Ä‘áº§u cá»§a cÃ¢u há»i vÃ  vá»‹ trÃ­ ngÄƒn cÃ¡ch giá»¯a 2 cÃ¢u. 2 token nÃ y sáº½ Ä‘Æ°á»£c dá»± bÃ¡o á»Ÿ output Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c pháº§n `Start/End Spand` cá»§a cÃ¢u output.

* **BÆ°á»›c 2**: CÃ¡c embedding vÃ©c tÆ¡ sau Ä‘Ã³ sáº½ Ä‘Æ°á»£c truyá»n vÃ o kiáº¿n trÃºc multi-head attention vá»›i nhiá»u block code (thÆ°á»ng lÃ  6, 12 hoáº·c 24 blocks tÃ¹y theo kiáº¿n trÃºc BERT). Ta thu Ä‘Æ°á»£c má»™t vÃ©c tÆ¡ output á»Ÿ encoder. 

* **BÆ°á»›c 3**: Äá»ƒ dá»± bÃ¡o phÃ¢n phá»‘i xÃ¡c suáº¥t cho tá»«ng vá»‹ trÃ­ tá»« á»Ÿ decoder, á»Ÿ má»—i time step chÃºng ta sáº½ truyá»n vÃ o decoder vÃ©c tÆ¡ output cá»§a encoder vÃ  vÃ©c tÆ¡ embedding input cá»§a decoder Ä‘á»ƒ tÃ­nh encoder-decoder attention (cá»¥ thá»ƒ vá» encoder-decoder attention lÃ  gÃ¬ cÃ¡c báº¡n xem láº¡i má»¥c 2.1.1). Sau Ä‘Ã³ projection qua liner layer vÃ  softmax Ä‘á»ƒ thu Ä‘Æ°á»£c phÃ¢n phá»‘i xÃ¡c suáº¥t cho output tÆ°Æ¡ng á»©ng á»Ÿ time step $t$.

* **BÆ°á»›c 4**: Trong káº¿t quáº£ tráº£ ra á»Ÿ output cá»§a transformer ta sáº½ cá»‘ Ä‘á»‹nh káº¿t quáº£ cá»§a cÃ¢u Question sao cho trÃ¹ng vá»›i cÃ¢u Question á»Ÿ input. CÃ¡c vá»‹ trÃ­ cÃ²n láº¡i sáº½ lÃ  thÃ nh pháº§n má»Ÿ rá»™ng `Start/End Span` tÆ°Æ¡ng á»©ng vá»›i cÃ¢u tráº£ lá»i tÃ¬m Ä‘Æ°á»£c tá»« cÃ¢u input.


LÆ°u Ã½ quÃ¡ trÃ¬nh huáº¥n luyá»‡n chÃºng ta sáº½ fine-tune láº¡i toÃ n bá»™ cÃ¡c tham sá»‘ cá»§a model BERT Ä‘Ã£ cut off top linear layer vÃ  huáº¥n luyá»‡n láº¡i tá»« Ä‘áº§u cÃ¡c tham sá»‘ cá»§a linear layer mÃ  chÃºng ta thÃªm vÃ o kiáº¿n trÃºc model BERT Ä‘á»ƒ customize láº¡i phÃ¹ há»£p vá»›i bÃ i toÃ¡n.

NhÆ° váº­y cÃ¡c báº¡n Ä‘Ã£ hÃ¬nh dung Ä‘Æ°á»£c model BERT Ä‘Æ°á»£c fine-tuning trong má»™t tÃ¡c vá»¥ nhÆ° tháº¿ nÃ o rá»“i chá»©? TÃ´i cÃ¡ ráº±ng qua quÃ¡ trÃ¬nh thá»±c hÃ nh á»Ÿ bÃ i sau cÃ¡c báº¡n sáº½ náº¯m vá»¯ng hÆ¡n cÃ¡ch thá»©c fine-tune BERT model.

## 2.3. Masked ML (MLM)

Masked ML lÃ  má»™t tÃ¡c vá»¥ cho phÃ©p chÃºng ta fine-tuning láº¡i cÃ¡c biá»ƒu diá»…n tá»« trÃªn cÃ¡c bá»™ dá»¯ liá»‡u unsupervised-text báº¥t ká»³. ChÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng Masked ML cho nhá»¯ng ngÃ´n ngá»¯ khÃ¡c nhau Ä‘á»ƒ táº¡o ra biá»ƒu diá»…n embedding cho chÃºng. CÃ¡c bá»™ dá»¯ liá»‡u cá»§a tiáº¿ng anh cÃ³ kÃ­ch thÆ°á»›c lÃªn tá»›i vÃ i vÃ i trÄƒm tá»›i vÃ i nghÃ¬n GB Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn BERT Ä‘Ã£ táº¡o ra nhá»¯ng káº¿t quáº£ khÃ¡ áº¥n tÆ°á»£ng.

BÃªn dÆ°á»›i lÃ  sÆ¡ Ä‘á»“ huáº¥n luyá»‡n BERT theo tÃ¡c vá»¥ Masked ML


<img src="/assets/images/20200523_BERTModel/pic5.png" class="largepic" />

**HÃ¬nh 5:** SÆ¡ Ä‘á»“ kiáº¿n trÃºc BERT cho tÃ¡ vá»¥ Masked ML.

Theo Ä‘Ã³:

* Khoáº£ng 15 % cÃ¡c token cá»§a cÃ¢u input Ä‘Æ°á»£c thay tháº¿ bá»Ÿi `[MASK]` token trÆ°á»›c khi truyá»n vÃ o model Ä‘áº¡i diá»‡n cho nhá»¯ng tá»« bá»‹ che dáº¥u (masked). MÃ´ hÃ¬nh sáº½ dá»±a trÃªn cÃ¡c tá»« khÃ´ng Ä‘Æ°á»£c che (non-masked) dáº¥u xung quanh `[MASK]` vÃ  Ä‘á»“ng thá»i lÃ  bá»‘i cáº£nh cá»§a `[MASK]` Ä‘á»ƒ dá»± bÃ¡o giÃ¡ trá»‹ gá»‘c cá»§a tá»« Ä‘Æ°á»£c che dáº¥u. Sá»‘ lÆ°á»£ng tá»« Ä‘Æ°á»£c che dáº¥u Ä‘Æ°á»£c lá»±a chá»n lÃ  má»™t sá»‘ Ã­t (15%) Ä‘á»ƒ tá»· lá»‡ bá»‘i cáº£nh chiáº¿m nhiá»u hÆ¡n (85%).

* Báº£n cháº¥t cá»§a kiáº¿n trÃºc BERT váº«n lÃ  má»™t mÃ´ hÃ¬nh seq2seq gá»“m 2 phase encoder giÃºp embedding cÃ¡c tá»« input vÃ  decoder giÃºp tÃ¬m ra phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a cÃ¡c tá»« á»Ÿ output. Kiáº¿n trÃºc Transfomer encoder Ä‘Æ°á»£c giá»¯ láº¡i trong tÃ¡c vá»¥ Masked ML. Sau khi thá»±c hiá»‡n self-attention vÃ  feed forward ta sáº½ thu Ä‘Æ°á»£c cÃ¡c vÃ©c tÆ¡ embedding á»Ÿ output lÃ  $O_1, O_2,..., O_5$

* Äá»ƒ tÃ­nh toÃ¡n phÃ¢n phá»‘i xÃ¡c suáº¥t cho tá»« output, chÃºng ta thÃªm má»™t Fully connect layer ngay sau Transformer Encoder. HÃ m softmax cÃ³ tÃ¡c dá»¥ng tÃ­nh toÃ¡n phÃ¢n phá»‘i xÃ¡c suáº¥t. Sá»‘ lÆ°á»£ng units cá»§a fully connected layer pháº£i báº±ng vá»›i kÃ­ch thÆ°á»›c cá»§a tá»« Ä‘iá»ƒn.

* Cuá»‘i cÃ¹ng ta thu Ä‘Æ°á»£c vÃ©c tÆ¡ nhÃºng cá»§a má»—i má»™t tá»« táº¡i vá»‹ trÃ­ MASK sáº½ lÃ  embedding vÃ©c tÆ¡ giáº£m chiá»u cá»§a vÃ©c tÆ¡ $O_i$ sau khi Ä‘i qua fully connected layer nhÆ° mÃ´ táº£ trÃªn hÃ¬nh váº½ bÃªn pháº£i.

HÃ m loss function cá»§a BERT sáº½ bá» qua máº¥t mÃ¡t tá»« nhá»¯ng tá»« khÃ´ng bá»‹ che dáº¥u vÃ  chá»‰ Ä‘Æ°a vÃ o máº¥t mÃ¡t cá»§a nhá»¯ng tá»« bá»‹ che dáº¥u. Do Ä‘Ã³ mÃ´ hÃ¬nh sáº½ há»™i tá»¥ lÃ¢u hÆ¡n nhÆ°ng Ä‘Ã¢y lÃ  Ä‘áº·c tÃ­nh bÃ¹ trá»« cho sá»± gia tÄƒng Ã½ thá»©c vá» bá»‘i cáº£nh. Viá»‡c lá»±a chá»n ngáº«u nhiÃªn 15% sá»‘ lÆ°á»£ng cÃ¡c tá»« bá»‹ che dáº¥u cÅ©ng táº¡o ra vÃ´ sá»‘ cÃ¡c ká»‹ch báº£n input cho mÃ´ hÃ¬nh huáº¥n luyá»‡n nÃªn mÃ´ hÃ¬nh sáº½ cáº§n pháº£i huáº¥n luyá»‡n ráº¥t lÃ¢u má»›i há»c Ä‘Æ°á»£c toÃ n diá»‡n cÃ¡c kháº£ nÄƒng.

## 2.4. Next Sentence Prediction (NSP)

ÄÃ¢y lÃ  má»™t bÃ i toÃ¡n phÃ¢n loáº¡i há»c cÃ³ giÃ¡m sÃ¡t vá»›i 2 nhÃ£n (hay cÃ²n gá»i lÃ  phÃ¢n loáº¡i nhá»‹ phÃ¢n). Input Ä‘áº§u vÃ o cá»§a mÃ´ hÃ¬nh lÃ  má»™t cáº·p cÃ¢u (pair-sequence) sao cho 50% cÃ¢u thá»© 2 Ä‘Æ°á»£c lá»±a chá»n lÃ  cÃ¢u tiáº¿p theo cá»§a cÃ¢u thá»© nháº¥t vÃ  50% Ä‘Æ°á»£c lá»±a chá»n má»™t cÃ¡ch ngáº«u nhiÃªn tá»« bá»™ vÄƒn báº£n mÃ  khÃ´ng cÃ³ má»‘i liÃªn há»‡ gÃ¬ vá»›i cÃ¢u thá»© nháº¥t. NhÃ£n cá»§a mÃ´ hÃ¬nh sáº½ tÆ°Æ¡ng á»©ng vá»›i `IsNext` khi cáº·p cÃ¢u lÃ  liÃªn tiáº¿p hoáº·c `NotNext` náº¿u cáº·p cÃ¢u khÃ´ng liÃªn tiáº¿p.

CÅ©ng tÆ°Æ¡ng tá»± nhÆ° mÃ´ hÃ¬nh Question and Answering, chÃºng ta cáº§n Ä‘Ã¡nh dáº¥u cÃ¡c vá»‹ trÃ­ Ä‘áº§u cÃ¢u thá»© nháº¥t báº±ng token `[CLS]` vÃ  vá»‹ trÃ­ cuá»‘i cÃ¡c cÃ¢u báº±ng token `[SEP]`. CÃ¡c token nÃ y cÃ³ tÃ¡c dá»¥ng nháº­n biáº¿t cÃ¡c vá»‹ trÃ­ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a tá»«ng cÃ¢u thá»© nháº¥t vÃ  thá»© hai.

<img src="/assets/images/20200523_BERTModel/pic1.png" class="largepic" />

**HÃ¬nh 6:** SÆ¡ Ä‘á»“ kiáº¿n trÃºc model BERT cho tÃ¡c vá»¥ NSP.

ThÃ´ng tin input Ä‘Æ°á»£c preprocessing trÆ°á»›c khi Ä‘Æ°a vÃ o mÃ´ hÃ¬nh huáº¥n luyá»‡n bao gá»“m:

* Ngá»¯ nghÄ©a cá»§a tá»« (token embeddings): ThÃ´ng qua cÃ¡c embedding vÃ©c tÆ¡ cho tá»«ng tá»«. CÃ¡c vÃ©c tÆ¡ Ä‘Æ°á»£c khá»Ÿi táº¡o tá»« pretrain model.

NgoÃ i embedding biá»ƒu diá»…n tá»« cá»§a cÃ¡c tá»« trong cÃ¢u, mÃ´ hÃ¬nh cÃ²n embedding thÃªm má»™t sá»‘ thÃ´ng tin:

* Loáº¡i cÃ¢u (segment embeddings): Gá»“m hai vÃ©c tÆ¡ lÃ  $E_A$ náº¿u tá»« thuá»™c cÃ¢u thá»© nháº¥t vÃ  $E_B$ náº¿u tá»« thuá»™c cÃ¢u thá»© hai.

* Vá»‹ trÃ­ cá»§a tá»« trong cÃ¢u (position embedding): lÃ  cÃ¡c vÃ©c tÆ¡ $E_0, ..., E_{10}$. TÆ°Æ¡ng tá»± nhÆ° positional embedding trong transformer.

VÃ©c tÆ¡ input sáº½ báº±ng tá»•ng cá»§a cáº£ ba thÃ nh pháº§n embedding theo `tá»«, cÃ¢u` vÃ  `vá»‹ trÃ­`.




# 3. CÃ¡c kiáº¿n trÃºc model BERT

Hiá»‡n táº¡i cÃ³ nhiá»u phiÃªn báº£n khÃ¡c nhau cá»§a model BERT. CÃ¡c phiÃªn báº£n Ä‘á»u dá»±a trÃªn viá»‡c thay Ä‘á»•i kiáº¿n trÃºc cá»§a Transformer táº­p trung á»Ÿ 3 tham sá»‘: $L$: sá»‘ lÆ°á»£ng cÃ¡c block sub-layers trong transformer, $H$: kÃ­ch thÆ°á»›c cá»§a embedding vÃ©c tÆ¡ (hay cÃ²n gá»i lÃ  hidden size), $A$: Sá»‘ lÆ°á»£ng head trong multi-head layer, má»—i má»™t head sáº½ thá»±c hiá»‡n má»™t self-attention. TÃªn gá»i cá»§a 2 kiáº¿n trÃºc bao gá»“m:

* $\textbf{BERT}_{\textbf{BASE}}(L=12, H=768, A=12)$: Tá»•ng tham sá»‘ 110 triá»‡u.

* $\textbf{BERT}_{\textbf{LARGE}}(L=24, H=1024, A=16)$: Tá»•ng tham sá»‘ 340 triá»‡u.

NhÆ° váº­y á»Ÿ kiáº¿n trÃºc BERT Large chÃºng ta tÄƒng gáº¥p Ä‘Ã´i sá»‘ layer, tÄƒng kÃ­ch thÆ°á»›c hidden size cá»§a embedding vÃ©c tÆ¡ gáº¥p 1.33 láº§n vÃ  tÄƒng sá»‘ lÆ°á»£ng head trong multi-head layer gáº¥p 1.33 láº§n.







# 4. Thá»±c hÃ nh model BERT
## 4.1. Giá»›i thiá»‡u vá» bÃ i toÃ¡n

ChÃºng ta sáº½ cÃ¹ng xÃ¢y dá»±ng má»™t á»©ng dá»¥ng Question and Answering cÃ³ chá»©c nÄƒng há»i Ä‘Ã¡p.

Dá»¯ liá»‡u bao gá»“m:

Input: Má»™t cáº·p cÃ¢u <Question, Paragraph>, Question lÃ  cÃ¢u há»i vÃ  Paragraph lÃ  Ä‘oáº¡n vÄƒn báº£n chá»©a cÃ¢u tráº£ lá»i cho cÃ¢u há»i.

Output: CÃ¢u tráº£ lá»i Ä‘Æ°á»£c trÃ­ch suáº¥t tá»« Paragraph.

ÄÃ¢y lÃ  má»™t á»©ng dá»¥ng khÃ¡ thÃº vá»¥ mÃ  tÃ´i Ä‘Ã£ compile thÃ nh cÃ´ng trÃªn thiáº¿t bá»‹ android. CÃ¡c báº¡n cÃ³ thá»ƒ download á»©ng dá»¥ng vá» vÃ  cháº¡y thá»­ nghiá»‡m [BERT - Tensorflow Lite - Khanh Blog](https://www.facebook.com/TowardDataScience/videos/201232064499053/).



Äá»ƒ thá»±c hiá»‡n tÃ¡c vá»¥ nÃ y tÃ´i sáº½ sá»­ dá»¥ng pretrain model tá»« package transformer. ChÃºng ta cÃ³ thá»ƒ cÃ i thÃ´ng qua cÃ¢u lá»‡nh bÃªn dÆ°á»›i.


```
!pip install transformers
```

    Collecting transformers
    [?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665kB 2.8MB/s 
    [?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)
    Collecting tokenizers==0.7.0
    [?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 14.6MB/s 
    [?25hRequirement already satisfied: dataclasses; python_version < "3.7" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)
    Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)
    Collecting sacremoses
    [?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 28.9MB/s 
    [?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)
    Collecting sentencepiece
    [?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 44.5MB/s 
    [?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)
    Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)
    Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)
    Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)
    Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)
    Building wheels for collected packages: sacremoses
      Building wheel for sacremoses (setup.py) ... [?25l[?25hdone
      Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=6a91001f4e422c7561a79667a70494bc9d94fdf57f29570ca9ac2f2f582a21ec
      Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45
    Successfully built sacremoses
    Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers
    Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0
    

## 4.2. XÃ¢y dá»±ng má»™t á»©ng dá»¥ng Question and Answering

CÃ¡c bÆ°á»›c dá»¯ liá»‡u:

* **Tokenize**: Táº¡o chuá»—i token lÃ  concatenate cá»§a cáº·p cÃ¢u `<Question, Paragraph>`, thÃªm cÃ¡c token `[CLS]` Ä‘Ã¡nh dáº¥u vá»‹ trÃ­ báº¯t Ä‘áº§u cÃ¢u `Question` vÃ  `[SEP]` Ä‘Ã¡nh dáº¥u vá»‹ trÃ­ káº¿t thÃºc cÃ¢u. Sau Ä‘Ã³ Tokenize toÃ n bá»™ cáº·p cÃ¢u `<Question, Paragraph>` thÃ nh chuá»—i index tá»« tá»« Ä‘iá»ƒn.

* **Set Segment IDs**: Táº¡o vÃ©c tÆ¡ segment cho cáº·p cÃ¢u `Question` vÃ  `Paragraph`. Trong Ä‘Ã³ index 0 Ä‘Ã¡nh dáº¥u cÃ¡c vá»‹ trÃ­ thuá»™c cÃ¢u A vÃ  index 1 Ä‘Ã¡nh dáº¥u cÃ¡c vá»‹ trÃ­ thuá»™c cÃ¢u B.

* **Evaluate**: Khá»Ÿi táº¡o model tá»« pretrain model `bert-large-uncased-whole-word-masking-finetuned-squad`. VÃ  dá»± bÃ¡o cÃ¡c vá»‹ trÃ­ `start` vÃ  `end` náº±m trong chuá»—i token.

* **Reconstruct Answer**:TrÃ­ch suáº¥t thÃ´ng tin cÃ¢u tráº£ lá»i.

Source code cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c tham kháº£o táº¡i [Question answering with fine tuned BERT](https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT)


```
from transformers import BertTokenizer
from transformers import BertForQuestionAnswering
import torch
# Initialize tokenizer for corpus of bert-large-uncased
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

# Initialize model BertForQuestionAnswering for bert-large-uncased
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

def answer_question(question, answer_text):
    '''
    Láº¥y input lÃ  chuá»—i string cá»§a cÃ¢u question vÃ  answer_text chá»©a ná»™i dung cÃ¢u tráº£ lá»i cá»§a cÃ¢u question.
    XÃ¡c Ä‘á»‹nh tá»« trong answer_text lÃ  cÃ¢u tráº£ lá»i vÃ  in ra.
    '''
    # ======== Tokenize ========
    # Ãp dá»¥ng tokenizer cho cáº·p cÃ¢u <question, answer_text>. input_ids lÃ  concatenate indice cá»§a cáº£ 2 cÃ¢u sau khi Ä‘Ã£ thÃªm cÃ¡c token CLS vÃ  SEP nhÆ° mÃ´ táº£ trong tÃ¡c vá»¥ Question and Answering.
    input_ids = tokenizer.encode(question, answer_text)

    # ======== Set Segment IDs ========
    # XÃ¡c Ä‘á»‹nh vá»‹ trÃ­ Ä‘áº§u tiÃªn chá»©a token [SEP] trong cÃ¢u.
    sep_index = input_ids.index(tokenizer.sep_token_id)

    # Táº¡o segment index Ä‘Ã¡nh dáº¥u cÃ¡c vá»‹ trÃ­ tá»« thuá»™c question (giÃ¡ trá»‹ 0) vÃ  answer_text (giÃ¡ trá»‹ 1)
    num_seg_a = sep_index + 1
    num_seg_b = len(input_ids) - num_seg_a
    segment_ids = [0]*num_seg_a + [1]*num_seg_b

    # Kiá»ƒm tra Ä‘á»™ dÃ i segment_ids pháº£i báº±ng input_ids
    assert len(segment_ids) == len(input_ids)

    # ======== Evaluate ========
    # Dá»± bÃ¡o phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a vá»‹ trÃ­ cá»§a tá»« start vÃ  tá»« end trong chuá»—i concatenate <question, answer_text> mÃ  chá»©a káº¿t quáº£ cho cÃ¢u tráº£ lá»i.
    start_scores, end_scores = model(torch.tensor([input_ids]), # chuá»—i index biá»ƒu thá»‹ cho inputs.
                                    token_type_ids=torch.tensor([segment_ids])) # chuá»—i index thÃ nh pháº§n segment cÃ¢u Ä‘á»ƒ phÃ¢n biá»‡t giá»¯a cÃ¢u question vÃ  cÃ¢u answer_text

    # ======== Reconstruct Answer ========
    # TÃ¬m ra vá»‹ trÃ­ start, end vá»›i score lÃ  cao nháº¥t
    answer_start = torch.argmax(start_scores)
    answer_end = torch.argmax(end_scores)

    # Chuyá»ƒn ngÆ°á»£c tá»« input_ids sang list tokens
    tokens = tokenizer.convert_ids_to_tokens(input_ids)

    # Token Ä‘áº§u tiÃªn cá»§a cÃ¢u tráº£ lá»i
    answer = tokens[answer_start]

    # Lá»±a chá»n cÃ¡c thÃ nh pháº§n cÃ²n láº¡i cá»§a cÃ¢u tráº£ lá»i vÃ  join chÃºng vá»›i whitespace.
    for i in range(answer_start + 1, answer_end + 1):
        
        # Náº¿u token lÃ  má»™t subword token (cÃ³ dáº¥u ## á»Ÿ Ä‘áº§u) thÃ¬ combine vÃ o answer báº±ng token gá»‘c (loáº¡i bá» dáº¥u ##).
        if tokens[i][0:2] == '##':
            answer += tokens[i][2:]
        
        # Náº¿u trÃ¡i láº¡i thÃ¬ combine trá»±c tiáº¿p vÃ o answer.
        else:
            answer += ' ' + tokens[i]
    print('Question: "' + question + '"')
    print('Answer: "' + answer + '"')
```

Thá»­ nghiá»‡m káº¿t quáº£ cá»§a mÃ´ hÃ¬nh trÃªn má»™t vÃ i cáº·p cÃ¢u `<Question, Paragraph>`.


```
question = "what is my dog name?"
paragraph = "I have a dog. It's name is Ricky. I get it at my 15th birthday, when it was a puppy."

answer_question(question, paragraph)
```

    Question: "what is my dog name?"
    Answer: "ricky"
    

Thá»­ nghiá»‡m má»™t vÄƒn báº£n khÃ¡c dÃ i hÆ¡n. TÃ´i sáº½ láº¥y má»™t Ä‘oáº¡n vÄƒn mÃ´ táº£ tiá»ƒu sá»­ cá»§a Ã´ng vua toÃ¡n há»c `Euler` vÃ  há»i thuáº­t toÃ¡n ngÃ y sinh cá»§a Ã´ng áº¥y. CÃ¡c báº¡n hÃ£y xem káº¿t quáº£ nhÃ©.


```
question = "when Leonhard Euler was born?"
paragraph = "Leonhard Euler: 15 April 1707 â€“ 18 September 1783 was a Swiss mathematician, \
physicist, astronomer, geographer, logician and engineer who made important and influential discoveries in many branches of mathematics, \
such as infinitesimal calculus and graph theory, \
while also making pioneering contributions to several branches such as topology and analytic number theory. \
He also introduced much of the modern mathematical terminology and notation, \
particularly for mathematical analysis, such as the notion of a mathematical function.[4] He is also known for his work in mechanics, fluid dynamics, optics, astronomy and music theory"

answer_question(question, paragraph)
```

    Question: "when Leonhard Euler was born?"
    Answer: "15 april 1707"
    

Ta cÃ³ thá»ƒ tháº¥y káº¿t quáº£ lÃ  chÃ­nh xÃ¡c.

Viá»‡c Ã¡p dá»¥ng pretrain model sáºµn cÃ³ trÃªn package transformer cho tÃ¡c vá»¥ `Question and Answering` lÃ  khÃ¡ dá»… dÃ ng. ChÃºng ta cÅ©ng cÃ³ thá»ƒ fine-tuning láº¡i cÃ¡c kiáº¿n trÃºc model question and answering cho dá»¯ liá»‡u Tiáº¿ng Viá»‡t Ä‘á»ƒ táº¡o ra cÃ¡c á»©ng dá»¥ng há»i Ä‘Ã¡p cho riÃªng mÃ¬nh. Äá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c Ä‘iá»u Ä‘Ã³ Ä‘Ã²i há»i pháº£i náº¯m vá»¯ng kiáº¿n trÃºc cá»§a model BERT Ä‘Æ°á»£c trÃ¬nh bÃ y trong bÃ i viáº¿t nÃ y. CÃ³ láº½ á»Ÿ má»™t bÃ i sau tÃ´i sáº½ hÆ°á»›ng dáº«n cÃ¡c báº¡n thá»±c hÃ nh Ä‘iá»u nÃ y.

# 5. Tá»•ng káº¿t

NhÆ° váº­y qua bÃ i nÃ y tÃ´i Ä‘Ã£ hÆ°á»›ng dáº«n cÃ¡c báº¡n kiáº¿n trÃºc tá»•ng quÃ¡t cá»§a model BERT vÃ  cÃ¡ch thá»©c Ã¡p dá»¥ng model BERT vÃ o trong cÃ¡c tÃ¡c vá»¥ down stream task trong NLP nhÆ° Masked ML, Next Sentence Prediction vÃ  thá»±c hÃ nh xÃ¢y dá»±ng má»™t á»©ng dá»¥ng Question and Answering ngay trÃªn pretrain model cá»§a transformer package.

CÃ¡c kiáº¿n trÃºc biáº¿n thá»ƒ má»›i cá»§a BERT hiá»‡n táº¡i váº«n Ä‘ang Ä‘Æ°á»£c nghiÃªn cá»©u vÃ  tiáº¿p tá»¥c phÃ¡t triá»ƒn nhÆ° [ROBERTA](https://huggingface.co/transformers/model_doc/roberta.html), [ALBERT](https://huggingface.co/transformers/model_doc/albert.html), [CAMEBERT](https://huggingface.co/transformers/model_doc/camembert.html), [XLMROBERTA](https://huggingface.co/transformers/model_doc/xlmroberta.html), ...

NgÃ y cÃ ng cÃ³ nhiá»u cÃ¡c pretrain model trÃªn BERT Ã¡p dá»¥ng cho nhiá»u ngÃ´n ngá»¯ khÃ¡c nhau trÃªn toÃ n tháº¿ giá»›i vÃ  táº¡o ra má»™t sá»± Ä‘á»™t phÃ¡ trong NLP. NgÃ´n ngá»¯ Tiáº¿ng Viá»‡t cá»§a chÃºng ta cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c VinAI nghiÃªn cá»©u vÃ  huáº¥n luyá»‡n pretrain model thÃ nh cÃ´ng. Báº¡n Ä‘á»c muá»‘n sá»­ dá»¥ng pretrain model nÃ y trong cÃ¡c tÃ¡c vá»¥ NLP cÃ³ thá»ƒ tham kháº£o thÃªm táº¡i [PhoBERT](https://github.com/VinAIResearch/PhoBERT).

# 6. TÃ i liá»‡u

1. [From word embeddings to Pretrained Language models](https://towardsdatascience.com/from-word-embeddings-to-pretrained-language-models-a-new-age-in-nlp-part-2-e9af9a0bdcd9)

2. [BERT explained state of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)

3. [huggingface - transformer github package](https://github.com/huggingface/transformers/)

4. [question answering with a fine tuned BERT](https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT)

5. [BERT fine-tuning with cloud](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)

6. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

7. [OpenAI GPT - paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

8. [ULMFit paper](https://arxiv.org/abs/1801.06146)
